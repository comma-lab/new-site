<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Communication Acoustics Lab</title> <meta name="author" content="Communication Acoustics Lab, Queen Mary University of London"/> <meta name="description" content="The Communication Acoustics Lab (COMMA) is part of the Centre for Digital Music (C4DM) at Queen Mary University of London, conducting cutting edge research into the ways people perceive sound and technologies for improving communication. "/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link href='https://fonts.googleapis.com/css?family=Inter Tight:400,500,600,700' rel='stylesheet'> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/logos/comma-logo-black.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="http://comma.eecs.qmul.ac.uk//publications/"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <img src="/assets/img/logos/comma-logo.png" alt="COMMA Lab logo" width="22vw">   <a class="navbar-brand title font-weight-lighter" href="/">Communication Acoustics Lab</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">People</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <center> <h2 class="post-title">Publications</h2> </center> <p class="post-description"></p> </header> <article> <div class="publications"> <h4 class="year">2025</h4> <ol class="bibliography"> <li> <div class="row"> <div id="charis_OHOS_2025" class="col-12"> <div class="title">Hearing brightness: Multidisciplinary perspectives on a ubiquitous attribute of timbre and orchestration (in press)</div> <div class="author"> Charalampos Saitis, Kai Siedenburg, and Moe Touizrar</div> <div class="periodical"> <em>The Oxford Handbook of Orchestration Studies</em>, 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Orchestration can have undeniable effects on what listeners hear as changes in auditory brightness. But the precise source of our sensation of brightness in orchestral music is difficult to pinpoint. What is the locus of brightness in terms of perception and orchestration? Moreover, how do separate notions of timbre and orchestration spanning music theory, composition, acoustics, and psychology each offer possible explanations? In this chapter, we attempt to articulate and distinguish between two modes: timbral brightness—the psychophysical perception of brightness in a given instance of sound, and orchestral brightness—the experience of brightness in the configuration and unfolding of orchestral music. We approach the central question from a multidisciplinary vantage, attempting to hold any tensions between the two modes without resolution.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="franco_jaes_2025" class="col-12"> <div class="title">Designing neural synthesizers for low latency interaction (in press)</div> <div class="author"> Franco Caspe, Jordie Shier, Mark Sandler, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Charalampos Saitis, Andrew McPherson' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Journal of the Audio Engineering Society</em>, 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Forthcoming.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="haokun_icassp_2025" class="col-12"> <div class="title">Hybrid losses for hierarchical embedding learning</div> <div class="author"> Haokun Tian, Stefan Lattner, Brian McFee, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Charalampos Saitis' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>50th IEEE International Conference on Acoustics, Speech and Signal Processing</em>, 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2501.12796" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/tiianhk/label-metric" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>In traditional supervised learning, cross-entropy loss penalises all incorrect predictions equally, failing to account for the relevance or proximity of wrong labels to the correct answer. Leveraging tree hierarchy for fine-grained labels, we investigate hybrid losses, including generalised triplet and cross-entropy losses, to impose inter-label similarity within a multi-task learning framework. We propose metrics to evaluate the embedding space structure and to test the generalisation ability for unseen classes, that is, whether the model can guess a similar class for data of an unseen class. Our experiments were conducted on OrchideaSOL, a four-level hierarchical instrument sound dataset with nearly 200 fine-grained categories. The proposed hybrid losses outperform previous work in classification, retrieval, embedding space structure, and generalisation.</p> </div> </div> </div> </li> </ol> <h4 class="year">2024</h4> <ol class="bibliography"> <li> <div class="row"> <div id="Colored_timbres_MTK_2024-2" class="col-12"> <div class="title">Colored timbres: Do crossmodal correspondences between musical instrument sounds and visual colors rather depend on pitch instead of timbre?</div> <div class="author"> Saleh Siddiq, Isabella Czedik-Eysenberg, Jörg Jewanski, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Charalampos Saitis, Sascha Kruchten, Rustem Sakhabiev, Michael Oehler, Christoph Reuter' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>Musik-, Tanz- &amp; Kunsttherapie</em>, 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Colored_timbres_MTK_2024-2.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Crossmodal correspondences between music and color gained much attention from researchers. Especially mappings of pitch to colors were investigated, while mappings of timbre to color garnered less interest. A short review over historic and recent studies on timbre-color mappings is given. An empirical study with 40 participants who were asked to match 60 musical instrument sounds with 38 colors was conducted in order to shed light on the underlying factors of the assignment of musical instruments’ sounds to colors. The question is if timbre-color matchings depend on timbre or if participants indeed resort to pitch as dominating factor. Despite notable interindividual inconsistencies, the results reflect some of the more common associations of colors and musical instruments, e.g., red/yellow and trumpet sounds. However, the influence of timbre was found to be less robust than the influence of pitch. The most reliable relation was the well known tendency to match lower pitches with darker colors and higher pitches with bright colors. As starting point for future research, the results were qualitatively compared with the sensations of one synesthete.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="charis_chime_2024" class="col-12"> <div class="title">Ethnographic exploration of timbre in hackathon designs</div> <div class="author"> Charalampos Saitis, Bleiz Macsen Del Sette, Jordie Shier, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Haokun Tian, Shuoyang Zheng, Sophie Skach, Courtney N Reed, Corey Ford' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>Annual Workshop of the Music and Human-Computer Interaction Networks (CHIME)</em>, 2024 </div> <div class="links"> <a href="https://static1.squarespace.com/static/6227c31a43daf21135453605/t/6734b6d5b882961f8ec04316/1731507925478/6+Charalampos+Saitis+et+al.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Abs</a> </div> <div class="abstract hidden"> <p>https://static1.squarespace.com/static/6227c31a43daf21135453605/t/6734b6d5b882961f8ec04316/1731507925478/6+Charalampos+Saitis+et+al.pdf</p> </div> </div> </div> </li> <li> <div class="row"> <div id="huge_survey_foundation_models_2024" class="col-12"> <div class="title">Foundation Models for Music: A Survey</div> <div class="author"> Yinghao Ma, Anders Øland, Anton Ragni, and <span class="more-authors" title="click to view 40 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '40 more authors' ? 'Bleiz MacSen Del Sette, Charalampos Saitis, Chris Donahue, Chenghua Lin, Christos Plachouras, Emmanouil Benetos, Elio Quinton, Elona Shatri, Fabio Morreale, Ge Zhang, György Fazekas, Gus Xia, Huan Zhang, Ilaria Manco, Jiawen Huang, Julien Guinot, Liwei Lin, Luca Marinelli, Max W. Y. Lam, Megha Sharma, Qiuqiang Kong, Roger B. Dannenberg, Ruibin Yuan, Shangda Wu, Shih-Lun Wu, Shuqi Dai, Shun Lei, Shiyin Kang, Simon Dixon, Wenhu Chen, Wenhao Huang, Xingjian Du, Xingwei Qu, Xu Tan, Yizhi Li, Zeyue Tian, Zhiyong Wu, Zhizheng Wu, Ziyang Ma, Ziyu Wang' : '40 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">40 more authors</span> </div> <div class="periodical"> <em>arXiv</em>, 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2408.14340" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> <div class="abstract hidden"> <p>In recent years, foundation models (FMs) such as large language models (LLMs) and latent diffusion models (LDMs) have profoundly impacted diverse sectors, including music. This comprehensive review examines state-of-the-art (SOTA) pre-trained models and foundation models in music, spanning from representation learning, generative learning and multimodal learning. We first contextualise the significance of music in various industries and trace the evolution of AI in music. By delineating the modalities targeted by foundation models, we discover many of the music representations are underexplored in FM development. Then, emphasis is placed on the lack of versatility of previous methods on diverse music applications, along with the potential of FMs in music understanding, generation and medical application. By comprehensively exploring the details of the model pre-training paradigm, architectural choices, tokenisation, finetuning methodologies and controllability, we emphasise the important topics that should have been well explored, like instruction tuning and in-context learning, scaling law and emergent ability, as well as long-sequence modelling etc. A dedicated section presents insights into music agents, accompanied by a thorough analysis of datasets and evaluations essential for pre-training and downstream tasks. Finally, by underscoring the vital importance of ethical considerations, we advocate that following research on FM for music should focus more on such issues as interpretability, transparency, human responsibility, and copyright issues. The paper offers insights into future challenges and trends on FMs for music, aiming to shape the trajectory of human-AI collaboration in the music realm.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="charis_APP_2024" class="col-12"> <div class="title">Timbral brightness perception investigated through multimodal interference</div> <div class="author"> Charalampos Saitis, and Zachary Wallmark</div> <div class="periodical"> <em>Attention, Perception, &amp; Psychophysics</em>, 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/s13414-024-02934-2.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Brightness is among the most studied aspects of timbre perception. Psychoacoustically, sounds described as “bright” vs “dark” typically exhibit a high vs low frequency emphasis in the spectrum. However, relatively little is known about the neurocognitive mechanisms that facilitate these metaphors we listen with. Do they originate in universal magnitude representations common to more than one sensory modality? Triangulating three different interaction paradigms, we investigated using speeded classification whether intramodal, crossmodal, and amodal interference occurs when timbral brightness, as modeled by the centroid of the spectral envelope, and pitch height / visual brightness / numerical value processing are semantically congruent and incongruent. In four online experiments varying in priming strategy, onset timing, and response deadline, 189 total participants were presented a baseline stimulus (a pitch, grey square, or numeral) then asked to quickly identify a target stimulus that is higher/lower, brighter/darker, or greater/less than the baseline after being primed with a bright or dark synthetic harmonic tone. Results suggest that timbral brightness modulates the perception of pitch and possibly visual brightness, but not numerical value. Semantically incongruent pitch height-timbral brightness shifts produced significantly slower reaction time (RT) and higher error compared to congruent pairs. In the visual task, incongruent pairings of grey squares and tones elicited slower RTs than congruent pairings (in two experiments). No interference was observed in the number comparison task. These findings shed light on the embodied and multimodal nature of experiencing timbre.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="charis_ESCOM_2024" class="col-12"> <div class="title">Relating timbre perception to musical craft practice: an empirical ethnographic approach</div> <div class="author"> Charalampos Saitis, Bleiz Macsen Del Sette, Jordie Shier, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Haokun Tian' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Triennial Conference of the European Society for the Cognitive Sciences of Music</em>, 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://vimeo.com/977751000/79270c2e97?share=copy" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> </div> <div class="abstract hidden"> <p>In crafting musical expression, the digital instrument maker is required to manipulate digital, and increasingly AI, technology as an additional medium. This raises interesting but unexplored questions about the role and practice of timbre in the development and adoption of sound technologies and their surrounding sonic cultures and, conversely, their imprint on the perceptual experience of timbre. Previous empirical research studied how the latter relates to the creative practice of sound synthesis. Here we adopt an ethnographic approach to explore the relationship between timbre and broader creative and technological practices of digital lutherie. We aim to better understand how makers think about and engage with timbre, what current practices and technologies of instrument design enable timbre exploration during the creative craft process, and how this knowledge can expand and diversify our understanding of how timbre is perceived, represented, and generated. Reflexive thematic analysis is applied to structured interviews with 20 (minimum target) instrument makers from commercial, research, independent, and artistic backgrounds. Here both ‘instrument’ and ‘maker’ are broadly construed, including composers and performers who build bespoke instruments as well as live coders. Interviews were conducted remotely and lasted around 50 minutes. Preliminary findings suggest that the entanglement of timbre and musical craft practice takes several forms, including interactions with aesthetic values and acoustics of material, which can be described as occupying places across a space encompassing many different notions (subspaces) of timbre entangled with a wide range of epistemic instruments and sonic practices. Rather than being a limited scientific (and musical) idea rooted in the psychoacoustic “timbre space” model, timbre emerges in a dynamic relay between technology and creation. Our study thus presents an empirical ethnographic understanding of timbre from the maker’s perspective, informing future development of tools to assist timbre exploration in musical craft practice.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="charis_AudioMostly_2024" class="col-12"> <div class="title">Timbre Tools: Ethnographic perspectives on timbre and sonic cultures in hackathon designs</div> <div class="author"> Charalampos Saitis, Bleiz Macsen Del Sette, Jordie Shier, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Haokun Tian, Shuoyang Zheng, Sophie Skach, Courtney N Reed, Corey Ford' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>International Audio Mostly Conference</em>, 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/am24-23.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Timbre is a nuanced yet abstractly defined concept. Its inherently subjective qualities make it challenging to design and work with. In this paper, we propose to explore the conceptualisation and negotiation of timbre within the creative practice of timbre tool makers. To this end, we hosted a hackathon event and performed an ethnographic study to explore how participants engaged with the notion of timbre and how their conception of timbre was shaped through social interactions and technological encounters. We present individual descriptions of the design process of each team and reflect across our data to identify commonalities in the ways that timbre is understood and informed by sound technologies and their surrounding sonic cultures, e.g., by relating concepts of timbre to metaphors. We further current understanding by offering novel interdisciplinary and multimodal insights into understandings of timbre.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="vjosa_ISMIR_2024" class="col-12"> <div class="title">Automatic detection of moral values in music lyrics</div> <div class="author"> Vjosa Preniqi, Iacopo Ghinassi, Julia Ive, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Kyriaki Kalimeri, Charalampos Saitis' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>International Society for Music Information Retrieval Conference</em>, 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2407.18787" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> <div class="abstract hidden"> <p>Moral values play a fundamental role in how we evaluate information, make decisions, and form judgements around important social issues. The possibility to extract morality rapidly from lyrics enables a deeper understanding of our music-listening behaviours. Building on the Moral Foundations Theory (MFT), we tasked a set of transformer-based language models (BERT) fine-tuned on 2,721 synthetic lyrics generated by a large language model (GPT-4) to detect moral values in 200 real music lyrics annotated by two experts. We evaluate their predictive capabilities against a series of baselines including out-of-domain (BERT fine-tuned on MFT-annotated social media texts) and zero-shot (GPT-4) classification. The proposed models yielded the best accuracy across experiments, with an average F1 weighted score of 0.8. This performance is, on average, 5% higher than out-of-domain and zero-shot models. When examining precision in binary classification, the proposed models perform on average 12% higher than the baselines. Our approach contributes to annotation-free and effective lyrics morality learning, and provides useful insights into the knowledge distillation of LLMs regarding moral expression in music, and the potential impact of these technologies on the creative industries and musical culture.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="vjosa_MoralBERT_2024" class="col-12"> <div class="title">MoralBERT: A fine-tuned language model for capturing moral values in social discussions</div> <div class="author"> Vjosa Preniqi, Iacopo Ghinassi, Charalampos Saitis, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Kyriaki Kalimeri' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>ACM International Conference on Information Technology for Social Good</em>, 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2403.07678" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/vjosapreniqi/MoralBERT" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p></p> </div> </div> </div> </li> <li> <div class="row"> <div id="jincheng_MLSP_2024" class="col-12"> <div class="title">Composer style-specific symbolic music generation using vector quantized discrete diffusion models</div> <div class="author"> Jincheng Zhang, György Fazekas, and Charalampos Saitis</div> <div class="periodical"> <em>IEEE International Workshop on Machine Learning for Signal Processing</em>, 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Emerging Denoising Diffusion Probabilistic Models (DDPM) have become increasingly utilised because of promising results they have achieved in diverse generative tasks with continuous data, such as image and sound synthesis. Nonetheless, the success of diffusion models has not been fully extended to discrete symbolic music. We propose to combine a vector quantized variational autoencoder (VQ-VAE) and discrete diffusion models for the generation of symbolic music with desired composer styles. The trained VQ-VAE can represent symbolic music as a sequence of indexes that correspond to specific entries in a learned codebook. Subsequently, a discrete diffusion model is used to model the VQ-VAE’s discrete latent space. The diffusion model is trained to generate intermediate music sequences consisting of codebook indexes, which are then decoded to symbolic music using the VQ-VAE’s decoder. The evaluation results demonstrate our model can generate symbolic music with target composer styles that meet the given conditions with a high accuracy of 72.36%. Our code is available at [URL will be provided here].</p> </div> </div> </div> </li> <li> <div class="row"> <div id="charis_dgm_2024" class="col-12"> <div class="title">Von A bis U: Die Vokalität von Instrumentalklangfarben (From A to U: The vocality of instrumental timbres)</div> <div class="author"> Christoph Reuter, Charalampos Saitis, Isabella Czedik-Eysenberg, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Kai Siedenburg' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>40. Jahrestagung der Deutschen Gesellschaft für Musikpsychologie</em>, 2024 </div> <div class="links"> <a href="/assets/pdf/Reuter_DGM_2024.pdf" class="btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Reuter_DGM_2024.pdf</p> </div> </div> </div> </li> <li> <div class="row"> <div id="luca_plos_preprint" class="col-12"> <div class="title">A multimodal understanding of the role of sound and music in gendered toy marketing</div> <div class="author"> Luca Marinelli, Petra Lucht, and Charalampos Saitis</div> <div class="periodical"> <em>PLOS ONE</em>, 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://github.com/marinelliluca/music-role-gender-marketing" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0311876" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Link</a> </div> <div class="abstract hidden"> <p>Literature in music theory and psychology shows that, even in isolation, musical sounds can reliably encode gender-loaded messages. In fact, musical material can be imbued with many ideological dimensions and gender is just one of them. Nonetheless, studies of the gendering of music within multimodal communicative events are sparse and lack an encompassing theoretical framework. The present study attempts to address this literature gap by means of a critical quantitative analysis of music in gendered toy marketing, which integrated a content analytical approach with multimodal affective and music-focused perceptual responses. Ratings were collected on a set of 606 commercials spanning over a ten years time frame, and strong gender polarisation was observed in nearly all of the collected variables. Gendered music styles in toy commercials were found to exhibit synergistic design choices, as music in masculine-targeted adverts was substantially more abrasive—louder, more inharmonious, and more distorted—than that in feminine-targeted ones. Toy advertising music appeared thus to be deliberately and consistently in line with traditional gender norms. In addition, music perceptual scales and voice-related content analytical variables were found to explain quite well the heavily polarised affective ratings. This study presents an empirical understanding of the gendering of music as constructed within multimodal discourse, reiterating the importance of the sociocultural underpinnings of music cognition. We provide a public repository with all code and data necessary to reproduce the results of this study at github.com/marinelliluca/music-role-gender-marketing.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="jordie_NIME_2024" class="col-12"> <div class="title">Real-time timbre remapping with differentiable DSP</div> <div class="author"> Jordie Shier, Charalampos Saitis, Andrew Robertson, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Andrew McPherson' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>International Conference on New Interfaces for Musical Expression</em>, 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://jordieshier.com/projects/nime2024/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://jordieshier.com/projects/nime2024/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Link</a> </div> <div class="abstract hidden"> <p>Timbre is a primary mode of expression in diverse musical contexts. However, prevalent audio-driven synthesis methods predominantly rely on pitch and loudness envelopes, effectively flattening timbral expression from the input. Our approach draws on the concept of timbre analogies and investigates how timbral expression from an input signal can be mapped onto controls for a synthesizer. Leveraging differentiable digital signal processing, our method facilitates direct optimization of synthesizer parameters through a novel feature difference loss. This loss function, designed to learn relative timbral differences between musical events, prioritizes the subtleties of graded timbre modulations within phrases, allowing for meaningful translations in a timbre space. Using snare drum performances as a case study, where timbral expression is central, we demonstrate real-time timbre remapping from acoustic snare drums to a differentiable synthesizer modeled after the Roland TR-808.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="shuoyang_NIME_2024" class="col-12"> <div class="title">Building sketch-to-sound mapping with unsupervised feature extraction and interactive machine learning</div> <div class="author"> Shuoyang Zheng, Bleiz M. Del Sette, Charalampos Saitis, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Anna Xambó, Nick Bryan-Kinns' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>International Conference on New Interfaces for Musical Expression</em>, 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/jasper-zheng/unsupervised-sketch-to-sound" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>In this paper, we explore the interactive construction and exploration of mappings between visual sketches and musical controls. Interactive Machine Learning (IML) allows creators to construct mappings with personalised training examples. However, when it comes to high-dimensional data such as sketches, dimensionality reduction techniques are required to extract features for the IML model. We propose using unsupervised machine learning to encode sketches into lower-dimensional latent representations, which are then used as the source for the IML model to construct sketch-to-sound mappings. We build a proof-of-concept prototype and demonstrate it using two compositions. We reflect on the composing processes to discuss the controllability and explorability in mappings built by this approach and how they contribute to the musical expression.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="alex_haokun_AES_AImusician" class="col-12"> <div class="title">Deep learning-based audio representations for the analysis and visualisation of electronic dance music DJ mixes</div> <div class="author"> Alexander Williams, Haokun Tian, Stefan Lattner, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Mathieu Barthet, Charalampos Saitis' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>AES International Symposium on AI and the Musician</em>, 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/DJ_Timbre_Tool_AES_Paper.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/alexjameswilliams/EDMAudioRepresentations" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Electronic dance music (EDM), produced using computers and electronic instruments, is a collection of musical subgenres that emphasise timbre and rhythm over melody and harmony. It is usually presented through the medium of DJing, where tracks are curated and mixed sequentially to offer unique listening and dancing experiences. However, unlike key and tempo annotations, DJs still rely on audition rather than metadata to examine and select tracks with complementary audio content. In this work, we investigate the use of deep learning-based representations (Complex Autoencoder and OpenL3) for analysing and visualising audio content on a corpus of DJ mixes with approximate transition timestamps and compare them with signal processing-based representations (joint time-frequency scattering transform and mel-frequency cepstral coefficients). Representations are computed once per second and visualised with UMAP dimensionality reduction. We propose heuristics based on the identification of observed patterns in visualisations and time-sensitive Euclidean distances in the representation space to compute DJ transition lengths, transition smoothness, and inter-song, song-to-song, and full-mix audio content consistency using audio representations along with rough DJ transition timestamps. Our method enables the visualisation of variations within music tracks, facilitating the analysis of DJ mixes and individual EDM tracks. This approach supports musicians in making informed creative decisions based on such visualisations. We share our code, dataset annotations, computed audio representations, and trained CAE model. We encourage researchers and music enthusiasts alike to analyse their own music using our tools: github.com/alexjameswilliams/EDMAudioRepresentations.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="montserrat_asa_2024" class="col-12"> <div class="title">Timbral effects of col legno tratto techniques on bowed cello sounds</div> <div class="author"> Montserrat Pàmies-Vilà, and Charalampos Saitis</div> <div class="periodical"> <em>186th Meeting of the Acoustical Society of America/Acoustics Week</em>, 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>There are several playing techniques for bowed-string instruments that make use of the wooden stick of the bow. The stick is quite often used to strike the strings gently (col legno battuto) and less commonly to bow on them (col legno tratto). Col legno has existed since the 17th century, and it is often used in modern compositions. When the stick is drawn across the string (tratto), the contact between the scrubbing stick and the string introduces noise. The player may choose to combine both hair and stick, depending on the desired sound. To evaluate the timbral effects of col legno tratto on the cello sound, the current study compares three variations across ordinary and contemporary bowing techniques: using only the hair, using both hair and stick, and using only the stick. Motion capture and audio-video recordings with expert cello players show how the bow tilt varies greatly between the three cases. Suitable audio descriptors of timbre are evaluated, which may help to correlate the observed playing parameters and sound properties with the semantic attributes used by experts to describe the timbre of these techniques.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="charis_daga_2024" class="col-12"> <div class="title">Giving instruments a voice: Are there vowel-like qualities in the timbres of musical instruments?</div> <div class="author"> Christoph Reuter, Charalampos Saitis, Isabella Czedik-Eysenberg, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Kai Siedenburg' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>50. Deutsche Jahrestagung für Akustik</em>, 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Reuter_DAGA_2024.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Scholars have long explored similarities between musical instrument sounds and vowel qualities of human voice sounds. From a psychoacoustic standpoint, however, this relationship remains poorly understood. Here, we seek to address whether musical instruments truly exhibit vowel-like qualities, whether specific instruments, registers, and dynamic levels stand out, and what the acoustical correlates of this relation might be. In an online experiment, German native speakers listen to the sounds of oboe, clarinet, flute, bassoon, trumpet, trombone, French horn, tuba, violin, viola, cello, and double bass in three registers and two dynamic levels. Their task is to assign the following vowels and umlauts (in German pronunciation) to instrument sounds: a, a (with overring), e, i, o, u, ä, ö, and ü. Furthermore, participants rate the strength of vowel similarity. Preliminary analyses (of 43 participants) suggest that although vowel similarity is rated approximately equally high, vowel associations do not seem to be equally consistent for different instruments. Particular similarity is observed between bassoon and tuba with the vowel o, oboe and violin with the vowel i. Audio features will be used to model vowel similarity.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="luca_icassp_xai_2024" class="col-12"> <div class="title">Explainable modeling of gender-targeting practices in toy advertising sound and music</div> <div class="author"> Luca Marinelli, and Charalampos Saitis</div> <div class="periodical"> <em>1st Workshop on Explainable Machine Learning for Speech and Audio, 49th IEEE International Conference on Acoustics, Speech and Signal Processing</em>, 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Luca_ICASSP_XAI_2024.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/marinelliluca/explainable-modeling" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://www.youtube.com/watch?v=JGpcIYRe4rA" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> </div> <div class="abstract hidden"> <p>This study examines gender coding in sound and music, in a context where music plays a supportive role to other modalities, such as in toy advertising. We trained a series of binary XGBoost classifiers on handcrafted features extracted from the soundtracks and then performed SAGE and SHAP analyses to identify key audio features in predicting the gender target of the ads. Our analysis reveals that timbral dimensions play a prominent role and that commercials aimed at girls tend to be more harmonious and rhythmical, with a broader and smoother spectrum, while those targeting boys are characterised by higher loudness, spectral entropy, and roughness. Mixed audience commercials instead appear to be as rhythmical as girls-only ads, although slower, but show intermediate characteristics in terms of harmonicity and roughness. This study highlights the importance of music in shaping societal norms and the need for greater accountability in its use in marketing and other industries. We provide a public repository containing all code and data used in this study.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="ben_DDSP_Review_2024" class="col-12"> <div class="title">A review of differentiable digital signal processing for music and speech synthesis</div> <div class="author"> Ben Hayes, Jordie Shier, György Fazekas, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Andrew McPherson, Charalampos Saitis' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Frontiers in Signal Processing</em>, 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Hayes_DDSP_Review.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The term differentiable digital signal processing describes a family of techniques in which loss function gradients are backpropagated through digital signal processors, facilitating their integration into neural networks. This article surveys the literature on differentiable audio signal processing, focusing on its use in music and speech synthesis. We catalogue applications to tasks including music performance rendering, sound matching, and voice transformation, discussing the motivations for and implications of the use of this methodology. This is accompanied by an overview of digital signal processing operations that have been implemented differentiably, which is further supported by a web book containing practical advice on differentiable synthesiser programming. Finally, we highlight open challenges, including optimisation pathologies, robustness to real-world conditions, and design trade-offs, and discuss directions for future research.</p> </div> </div> </div> </li> </ol> <h4 class="year">2023</h4> <ol class="bibliography"> <li> <div class="row"> <div id="bleiz_chime_2023" class="col-12"> <div class="title">A body-centred perspective to chronic pain self-management using generative sonification</div> <div class="author"> Bleiz Macsen Del Sette, and Charalampos Saitis</div> <div class="periodical"> <em>Annual Workshop of the Music and Human-Computer Interaction Networks (CHIME)</em>, 2023 </div> <div class="links"> <a href="/assets/pdf/DelSette_CHIME_2023.pdf" class="btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>DelSette_CHIME_2023.pdf</p> </div> </div> </div> </li> <li> <div class="row"> <div id="charis_chime_2023" class="col-12"> <div class="title">Timbre Tools for the Digital Instrument Maker</div> <div class="author"> Charalampos Saitis, Haokun Tian, Jordie Shier, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Bleiz Macsen Del Sette' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Annual Workshop of the Music and Human-Computer Interaction Networks (CHIME)</em>, 2023 </div> <div class="links"> <a href="/assets/pdf/Saitis_CHIME_2023.pdf" class="btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/CHIME_Poster_SAITIS.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>Saitis_CHIME_2023.pdf</p> </div> </div> </div> </li> <li> <div class="row"> <div id="haokun_ismir_2023" class="col-12"> <div class="title">Beat and Downbeat Tracking with Generative Embeddings</div> <div class="author"> Haokun Tian, Kun Liu, and Magdalena Fuentes</div> <div class="periodical"> <em>Late Breaking Demo of the 24nd International Society for Music Information Retrieval Conference</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://ismir2023program.ismir.net/lbd_363.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="http://ismir2023program.ismir.net/lbd_363.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a> </div> <div class="abstract hidden"> <p>It is standard practice to use spectrograms as input features for discriminative MIR tasks. However, recent research showed using representations produced by Jukebox (a music language model) led to better model performance. This was tested on music tagging, genre classification, key detection, emotion recognition, and music transcription. In this paper, we test it on beat and downbeat tracking. Specifically, we compare compressed Jukebox embeddings with spectrograms as input to a model that jointly predicts beat, downbeat, and tempo. Experiments show that the two inputs bring comparable results for beat tracking, while using Jukebox embeddings leads to significant improvements for downbeat tracking.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="vjosa_soundscapes_2023" class="col-12"> <div class="title">Soundscapes of morality: Linking music preferences and moral values through lyrics and audio</div> <div class="author"> Vjosa Preniqi, Kyriaki Kalimeri, and Charalampos Saitis</div> <div class="periodical"> <em>PLOS ONE</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Preniqi_Soundscapes-of-Morality.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Music is a fundamental element in every culture, serving as a universal means of expressing our emotions, feelings, and beliefs. This work investigates the link between our moral values and musical choices through lyrics and audio analyses. We align the psychometric scores of 1,480 participants to acoustics and lyrics features obtained from the top 5 songs of their preferred music artists from Facebook Page Likes. We employ a variety of lyric text processing techniques, including lexicon-based approaches and BERT-based embeddings, to identify each song’s narrative, moral valence, attitude, and emotions. In addition, we extract both low- and high-level audio features to comprehend the encoded information in participants’ musical choices and improve the moral inferences. We propose a Machine Learning approach and assess the predictive power of lyrical and acoustic features separately and in a multimodal framework for predicting moral values. Results indicate that lyrics and audio features from the artists people like inform us about their morality. Though the most predictive features vary per moral value, the models that utilised a combination of lyrics and audio characteristics were the most successful in predicting moral values, outperforming the models that only used basic features such as user demographics, the popularity of the artists, and the number of likes per user. Audio features boosted the accuracy in the prediction of empathy and equality compared to textual features, while the opposite happened for hierarchy and tradition, where higher prediction scores were driven by lyrical features. This demonstrates the importance of both lyrics and audio features in capturing moral values. The insights gained from our study have a broad range of potential uses, including customising the music experience to meet individual needs, music rehabilitation, or even effective communication campaign crafting.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="vjosa_cmmr-book_2023" class="col-12"> <div class="title">Modelling Moral Traits with Music Listening Preferences and Demographics</div> <div class="author"> Vjosa Preniqi, Kyriaki Kalimeri, and Charalampos Saitis</div> <div class="periodical"> <em>Music in the AI Era. CMMR 2021. Springer Lecture Notes in Computer Science, vol 13770</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Preniqi_CMMR_SpringerLNCS_2023.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Music has always been an integral part of our everyday lives through which we express feelings, emotions, and concepts. Here, we explore the association between music genres, demographics and moral values employing data from an ad-hoc online survey and the Music Learning Histories Dataset. To further characterise the music preferences of the participants the generalist/specialist (GS) score employed. We exploit both classification and regression approaches to assess the predictive power of music preferences for the prediction of demographic attributes as well as the moral values of the participants. Our findings point out that moral values are hard to predict (.62 AUROC_avg) solely by the music listening behaviours, while if basic sociodemographic information is provided the prediction score rises to 4% on average (.66 AUROC_avg), with the Purity foundation to be the one that is steadily the one with the highest accuracy scores. Similar results are obtained from the regression analysis. Finally, we provide with insights on the most predictive music behaviours associated with each moral value that can inform a wide range of applications from rehabilitation practices to communication campaign design.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="jincheng_preprint_2023_b" class="col-12"> <div class="title">Fast Diffusion GAN Model for Symbolic Music Generation Controlled by Emotions</div> <div class="author"> Jincheng Zhang, György Fazekas, and Charalampos Saitis</div> <div class="periodical"> <em>arXiv</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2310.14040" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> <div class="abstract hidden"> <p>Diffusion models have shown promising results for a wide range of generative tasks with continuous data, such as image and audio synthesis. However, little progress has been made on using diffusion models to generate discrete symbolic music because this new class of generative models are not well suited for discrete data while its iterative sampling process is computationally expensive. In this work, we propose a diffusion model combined with a Generative Adversarial Network, aiming to (i) alleviate one of the remaining challenges in algorithmic music generation which is the control of generation towards a target emotion, and (ii) mitigate the slow sampling drawback of diffusion models applied to symbolic music generation. We first used a trained Variational Autoencoder to obtain embeddings of a symbolic music dataset with emotion labels and then used those to train a diffusion model. Our results demonstrate the successful control of our diffusion model to generate symbolic music with a desired emotion. Our model achieves several orders of magnitude improvement in computational cost, requiring merely four time steps to denoise while the steps required by current state-of-the-art diffusion models for symbolic music generation is in the order of thousands.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="bleiz_cscw_2023" class="col-12"> <div class="title">Sound of Care: Towards a Co-Operative AI Digital Pain Companion to Support People with Chronic Primary Pain</div> <div class="author"> Bleiz Macsen Del Sette, Dawn Carnes, and Charalampos Saitis</div> <div class="periodical"> <em>Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://uco.repository.guildhe.ac.uk/id/eprint/199/1/Sound%20of%20Care.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="/assets/pdf/CSCW23-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>This work investigates the role of sound and technology in the everyday lives of people with chronic primary pain. Our primary goal was to inform the first participatory design workshop of Sound of Care, a new eHealth system for pain self-management. We used an ethical stakeholder analysis to inform a round of exploratory interviews, run with 8 participants including people with chronic primary pain, carers, and healthcare workers. We found that sound and technology serve as important but often unstructured tool, helping with distraction, mood regulation and sleep. The experience of pain and musical preferences are highly personal, and communicating or understanding pain can be challenging, even within family members. To address the gaps in current chronic pain self-management care, we propose the use a sound-based AI-driven system, a Digital Pain Companion, using sonification to create a shared decision-making space, enhancing agency over treatment in a co-operative care environment.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="luca_ismir_2023" class="col-12"> <div class="title">Gender-Coded Sound: Analysing the Gendering of Music in Toy Commercials via Multi-Task Learning</div> <div class="author"> Luca Marinelli, György Fazekas, and Charalampos Saitis</div> <div class="periodical"> <em>24th International Society for Music Information Retrieval Conference</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.researchgate.net/profile/Luca-Marinelli-5/publication/372279840_Gender-Coded_Sound_Analysing_the_Gendering_of_Music_in_Toy_Commercials_via_Multi-Task_Learning/links/64ad781295bbbe0c6e2cc1ec/Gender-Coded-Sound-Analysing-the-Gendering-of-Music-in-Toy-Commercials-via-Multi-Task-Learning.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://www.youtube.com/watch?v=d96uEzo6VG4" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> </div> <div class="abstract hidden"> <p>Music can convey ideological stances, and gender is just one of them. Evidence from musicology and psychology research shows that gender-loaded messages can be reliably encoded and decoded via musical sounds. However, much of this evidence comes from examining music in isolation, while studies of the gendering of music within multimodal communicative events are sparse. In this paper, we outline a method to automatically analyse how music in TV advertising aimed at children may be deliberately used to reinforce traditional gender roles. Our dataset of 606 commercials included music-focused mid-level perceptual features, multimodal aesthetic emotions, and content analytical items. Despite its limited size, and because of the extreme gender polarisation inherent in toy advertisements, we obtained noteworthy results by leveraging multi-task transfer learning on our densely annotated dataset. The models were trained to categorise commercials based on their intended target audience, specifically distinguishing between masculine, feminine, and mixed audiences. Additionally, to provide explainability for the classification in gender targets, the models were jointly trained to perform regressions on emotion ratings across six scales, and on mid-level musical perceptual attributes across twelve scales. Standing in the context of MIR, computational social studies and critical analysis, this study may benefit not only music scholars but also advertisers, policymakers, and broadcasters.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="jordie_FA_2023" class="col-12"> <div class="title">Differentiable Modelling of Percussive Audio with Transient and Spectral Synthesis</div> <div class="author"> Jordie Shier, Franco Caspe, Andrew Robertson, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Mark Sandler, Charalampos Saitis, Andrew McPherson' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>10th Convention of the European Acoustics Association</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://jordieshier.com/assets/pdf/shier2023differentiable_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Differentiable digital signal processing (DDSP) techniques, including methods for audio synthesis, have gained attention in recent years and lend themselves to interpretability in the parameter space. However, current differentiable synthesis methods have not explicitly sought to model the transient portion of signals, which is important for percussive sounds. In this work, we present a unified synthesis framework aiming to address transient generation and percussive synthesis within a DDSP framework. To this end, we propose a model for percussive synthesis that builds on sinusoidal modeling synthesis and incorporates a modulated temporal convolutional network for transient generation. We use a modified sinusoidal peak picking algorithm to generate time-varying non-harmonic sinusoids and pair it with differentiable noise and transient encoders that are jointly trained to reconstruct drumset sounds. We compute a set of reconstruction metrics using a large dataset of acoustic and electronic percussion samples that show that our method leads to improved onset signal reconstruction for membranophone percussion instruments.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="ben_iclr_2023" class="col-12"> <div class="title">The Responsibility Problem in Neural Networks with Unordered Targets</div> <div class="author"> Ben Hayes, Charalampos Saitis, and György Fazekas</div> <div class="periodical"> <em>11th International Conference on Learning Representations, Tiny Papers</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2304.09499" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> <div class="abstract hidden"> <p>We discuss the discontinuities that arise when mapping unordered objects to neural network outputs of fixed permutation, referred to as the responsibility problem. Prior work has proved the existence of the issue by identifying a single discontinuity. Here, we show that discontinuities under such models are uncountably infinite, motivating further research into neural networks for unordered data.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="rodrigo_nime_2023" class="col-12"> <div class="title">Interactive Neural Resonators</div> <div class="author"> Rodrigo Diaz, Charalampos Saitis, and Mark Sandler</div> <div class="periodical"> <em>International Conference on New Interfaces for Musical Expression</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2305.14867" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://interactive-neural-resonators.com/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> </div> <div class="abstract hidden"> <p>In this work, we propose a method for the controllable synthesis of real-time contact sounds using neural resonators. Previous works have used physically inspired statistical methods and physical modelling for object materials and excitation signals. Our method incorporates differentiable second-order resonators and estimates their coefficients using a neural network that is conditioned on physical parameters. This allows for interactive dynamic control and the generation of novel sounds in an intuitive manner. We demonstrate the practical implementation of our method and explore its potential creative applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="vjosa_ic2s2_2023" class="col-12"> <div class="title">Gender differences in Moral Valence, Sentiment, and Narratives of Song Lyrics Over Time</div> <div class="author"> Vjosa Preniqi, Kyriaki Kalimeri, Andreas Kaltenbrunner, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Charalampos Saitis' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>9th International Conference on Computational Social Science</em>, 2023 </div> <div class="links"> <a href="/assets/pdf/vjosa_IC2S2_23.pdf" class="btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>vjosa_IC2S2_23.pdf</p> </div> </div> </div> </li> <li> <div class="row"> <div id="luca_icmpc_2023" class="col-12"> <div class="title">Analysing the Gendering of Music in Toy Commercials via Mid-level Perceptual Features</div> <div class="author"> Luca Marinelli, and Charalampos Saitis</div> <div class="periodical"> <em>17th International Conference on Music Perception and Cognition</em>, 2023 </div> <div class="links"> <a href="/assets/pdf/Luca_ICMPC23.pdf" class="btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Luca_ICMPC23.pdf</p> </div> </div> </div> </li> <li> <div class="row"> <div id="vjosa_icmpc_2023a" class="col-12"> <div class="title">Exploring the Role of Audio and Lyrics in Explaining Moral Worldviews</div> <div class="author"> Vjosa Preniqi, Kyriaki Kalimeri, and Charalampos Saitis</div> <div class="periodical"> <em>17th International Conference on Music Perception and Cognition</em>, 2023 </div> <div class="links"> <a href="/assets/pdf/vjosa_ICMPC_23_1.pdf" class="btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>vjosa_ICMPC_23_1.pdf</p> </div> </div> </div> </li> <li> <div class="row"> <div id="vjosa_icmpc_2023b" class="col-12"> <div class="title">Evolution of Moral Valence in Lyrics Over Time</div> <div class="author"> Vjosa Preniqi, Kyriaki Kalimeri, Andreas Kaltenbrunner, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Charalampos Saitis' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>17th International Conference on Music Perception and Cognition</em>, 2023 </div> <div class="links"> <a href="/assets/pdf/vjosa_ICMPC_23_2.pdf" class="btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>vjosa_ICMPC_23_2.pdf</p> </div> </div> </div> </li> <li> <div class="row"> <div id="charis_cgpt_timbre" class="col-12"> <div class="title">When ChatGPT Talks Timbre</div> <div class="author"> Charalampos Saitis, and Kai Siedenburg</div> <div class="periodical"> <em>3rd International Conference on Timbre</em>, 2023 </div> <div class="links"> <a href="/assets/pdf/3603_cam_ready.pdf" class="btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>3603_cam_ready.pdf</p> </div> </div> </div> </li> <li> <div class="row"> <div id="charis_nime_timbre" class="col-12"> <div class="title">When NIME and ISMIR Talk Timbre</div> <div class="author"> Charalampos Saitis, Maryam F. Torshizi, Vjosa Preniqi, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Bleiz M. Del Sette, György Fazekas' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>3rd International Conference on Timbre</em>, 2023 </div> <div class="links"> <a href="/assets/pdf/553_camera_ready.pdf" class="btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Saitis_timbre2023.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>553_camera_ready.pdf</p> </div> </div> </div> </li> <li> <div class="row"> <div id="charis_cgpt_arXiv" class="col-12"> <div class="title">The language of sounds unheard: Exploring musical timbre semantics of large language models</div> <div class="author"> Kai Siedenburg, and Charalampos Saitis</div> <div class="periodical"> <em>arXiv</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2304.07830" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> <div class="abstract hidden"> <p>Semantic dimensions of sound have been playing a central role in understanding the nature of auditory sensory experience as well as the broader relation between perception, language, and meaning. Accordingly, and given the recent proliferation of large language models (LLMs), here we asked whether such models exhibit an organisation of perceptual semantics similar to those observed in humans. Specifically, we prompted ChatGPT, a chatbot based on a state-of-the-art LLM, to rate musical instrument sounds on a set of 20 semantic scales. We elicited multiple responses in separate chats, analogous to having multiple human raters. ChatGPT generated semantic profiles that only partially correlated with human ratings, yet showed robust agreement along well-known psychophysical dimensions of musical sounds such as brightness (bright-dark) and pitch height (deep-high). Exploratory factor analysis suggested the same dimensionality but different spatial configuration of a latent factor space between the chatbot and human ratings. Unexpectedly, the chatbot showed degrees of internal variability that were comparable in magnitude to that of human ratings. Our work highlights the potential of LLMs to capture salient dimensions of human sensory experience.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="ben_icassp_2023" class="col-12"> <div class="title">Sinusoidal Frequency Estimation by Gradient Descent</div> <div class="author"> Ben Hayes, Charalampos Saitis, and György Fazekas</div> <div class="periodical"> <em>48th IEEE International Conference on Acoustics, Speech and Signal Processing</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2210.14476" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="/assets/pdf/Sinusoidal_Frequency_Estimation_by_Gradient_Descent.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Sinusoidal parameter estimation is a fundamental task in applications from spectral analysis to time-series forecasting. Estimating the sinusoidal frequency parameter by gradient descent is, however, often impossible as the error function is non-convex and densely populated with local minima. The growing family of differentiable signal processing methods has therefore been unable to tune the frequency of oscillatory components, preventing their use in a broad range of applications. This work presents a technique for joint sinusoidal frequency and amplitude estimation using the Wirtinger derivatives of a complex exponential surrogate and any first order gradient-based optimiser, enabling end-to-end training of neural network controllers for unconstrained sinusoidal models.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="rodrigo_icassp_2023" class="col-12"> <div class="title">Rigid-Body Sound Synthesis with Differentiable Modal Resonators</div> <div class="author"> Rodrigo Diaz, Ben Hayes, Charalampos Saitis, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'György Fazekas, Mark Sandler' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>48th IEEE International Conference on Acoustics, Speech and Signal Processing</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2210.15306" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="/assets/pdf/Rigid-Body_Sound_Synthesis_with_Differentiable_Modal_Resonators.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Physical models of rigid bodies are used for sound synthesis in applications from virtual environments to music production. Traditional methods, such as modal synthesis, often rely on computationally expensive numerical solvers, while recent deep learning approaches are limited by post-processing of their results. In this work, we present a novel end-to-end framework for training a deep neural network to generate modal resonators for a given 2D shape and material using a bank of differentiable IIR filters. We demonstrate our method on a dataset of synthetic objects but train our model using an audio-domain objective, paving the way for physically-informed synthesisers to be learned directly from recordings of real-world objects.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="charis_musperc_2023" class="col-12"> <div class="title">Timbre semantic associations vary both between and within instruments: An empirical study incorporating register and pitch height</div> <div class="author"> Lindsey Reymore, Jason Noble, Charalampos Saitis, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Caroline Traube, Zachary Wallmark' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Music Perception</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.mcgill.ca/mpcl/files/mpcl/reymore_2023_muspercept.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>The main objective of this study is to understand how timbre semantic associations — for example, a sound’s timbre perceived as bright, rough, or hollow — vary with register and pitch height across instruments. In this experiment, 540 online participants rated single, sustained notes from eight Western orchestral instruments (flute, oboe, bass clarinet, trumpet, trombone, violin, cello, and vibraphone) across three registers (low, medium, and high) on 20 semantic scales derived from Reymore and Huron (2020). The 24 two-second stimuli, equalized in loudness, were produced using the Vienna Symphonic Library. Exploratory modeling examined relationships between mean ratings of each semantic dimension and instrument, register, and participant musician identity (‘‘musician’’ vs. ‘‘nonmusician’’). For most semantic descriptors, both register and instrument were significant predictors, though the amount of variance explained differed (marginal R^2). Terms that had the strongest positive relationships with register include shrill/harsh/noisy, sparkling/brilliant/bright, ringing/long decay, and percussive. Terms with the strongest negative relationships with register include deep/thick/heavy, raspy/grainy/gravelly, hollow, and woody. Post hoc modeling using only pitch height and only register to predict mean semantic rating suggests that pitch height may explain more variance than does register. Results help clarify the influence of both instrument and relative register (and pitch height) on common timbre semantic associations.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="Timbre_2023_proc" class="col-12"> <div class="title">Proceedings of the 3rd International Conference on Timbre</div> <div class="author"> Eds: Marcelo Caetano, Zachary Wallmark, Asterios Zacharakis, and <span class="more-authors" title="click to view 2 more editors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more editors' ? 'Charalampos Saitis, Kai Siedenburg' : '2 more editors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more editors</span> </div> <div class="periodical"> The School of Music Studies, Aristotle University of Thessaloniki, 2023 </div> <div class="links"> <a href="https://drive.google.com/file/d/14AWI_ZL3LunXhO9DEmGznPAXnpYTKQhm/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Link</a> </div> </div> </div> </li> </ol> <h4 class="year">2022</h4> <ol class="bibliography"> <li> <div class="row"> <div id="jordie_dmrn_2022" class="col-12"> <div class="title">Real-time timbre mapping for synthesized percussive performance</div> <div class="author"> Jordie Shier</div> <div class="periodical"> <em>DMRN+17: Digital Music Research Network One-Day Workshop</em>, 2022 </div> <div class="links"> <a href="https://jordieshier.com/assets/pdf/shier2022dmrn_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a> </div> </div> </div> </li> <li> <div class="row"> <div id="vjosa_ismir_2022" class="col-12"> <div class="title">More Than Words: Linking Music Preferences and Moral Values Through Lyrics</div> <div class="author"> Vjosa Preniqi, Kyriaki Kalimeri, and Charalampos Saitis</div> <div class="periodical"> <em>23rd International Society for Music Information Retrieval Conference</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://archives.ismir.net/ismir2022/paper/000096.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://drive.google.com/file/d/1MMI5RbZdgjhQOavn7Slxzysb2RQ_Tza1/view" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> </div> <div class="abstract hidden"> <p>This study explores the association between music preferences and moral values by applying text analysis techniques to lyrics. Harvesting data from a Facebook-hosted application, we align psychometric scores of 1,386 users to lyrics from the top 5 songs of their preferred music artists as emerged from Facebook Page Likes. We extract a set of lyrical features related to each song’s overarching narrative, moral valence, sentiment, and emotion. A machine learning framework was designed to exploit regression approaches and evaluate the predictive power of lyrical features for inferring moral values. Results suggest that lyrics from top songs of artists people like inform their morality. Virtues of hierarchy and tradition achieve higher prediction scores (between .20 and .30) than values of empathy and equality (between .08 and .11), while basic demographic variables only account for a small part in the models’ explainability. This shows the importance of music listening behaviours, as assessed via lyrical preferences, alone in capturing moral values. We discuss the technological and musicological implications and possible future improvements.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="ben_ica_2022" class="col-12"> <div class="title">timbre.fun: A gamified interactive system for crowdsourcing a timbre semantic vocabulary</div> <div class="author"> Ben Hayes, Charalampos Saitis, and György Fazekas</div> <div class="periodical"> <em>24th International Congress on Acoustics</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/ICA_2022_template_final_ABS-0997.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We present timbre.fun, a web-based gamified interactive system where users create sounds in response to semantic prompts (e.g., bright, rough) through exploring a two-dimensional control space that maps nonlinearly to the parameters of a simple hybrid wavetable and amplitude-modulation synthesizer. The current version features 25 semantic adjectives mined from a popular synthesis forum. As well as creating sounds, users can explore heatmaps generated from others’ responses, and fit a classifier (k-nearest neighbors) in-browser. timbre.fun is based on recent work, including by the authors, which studied timbre semantic associations through prompted synthesis paradigms. The interactive is embedded in a digital exhibition on sensory variation and interaction (seeingmusic.app) which debuted at the 2021 Edinburgh Science Festival, where it was visited by 197 users from 21 countries over 16 days. As it continues running online, a further 596 visitors from 35 countries have engaged. To date 579 sounds have been created and tagged, which will facilitate parallel research in timbre semantics and neural audio synthesis. Future work will include further gamifying the data collection pipeline, including leveling-up to unlock new words and synthesizers, and a full open-source release.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="charis_imrf_2022" class="col-12"> <div class="title">Seeing Music: Leveraging citizen science and gamification to study cross-sensory associations</div> <div class="author"> Charalampos Saitis, Christine Cuskley, and Sebastian Löbbers</div> <div class="periodical"> <em>20th International Multisensory Research Forum</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Our recent research has shown that people lack knowledge about how the senses interact and are unaware of many common forms of sensory and perceptual variation. We present Seeing Music, a digital interactive exhibition and audiovisual game that translates high-level scientific understanding of sensory variation and cross-modality into knowledge for the public. Using a narrative-driven gamified approach, players are tasked with communicating human music to an extraterrestrial intelligence through visual shape, color and texture using two-dimensional selector panels. Music snippets (12–24 s long) are played continuously in a loop, taken from three custom instrumental compositions designed to vary systematically in terms of timbre, melody, and rhythm. Players can “level-up” to unlock new visual features and musical snippets, and explore and evaluate collaborative visualizations made by others. Outside the game, a series of interactive slideshows help visitors learn more about sensory experience, sensory diversity, and how our senses make us human. The exhibition debuted at the 2021 Edinburgh Science Festival, where it was visited by 197 users coming from 21 countries (134 visitors from the UK) over 16 days. As it continues running online, a further 596 visitors from 35 countries (164 from the UK) have engaged. To date, 169 players of Seeing Music have produced more than 42,500 audiovisual mapping datapoints for scientific research purposes. Preliminary analysis suggests that music with less high-frequency energy was mapped to less complex and rounder shapes, bluer and less bright hues, and less dense textures. These trends confirm auditory-visual correspondences previously reported in more controlled laboratory studies, while also offering new insight into how different auditory-visual associations interact with each other. Future work includes improving user motivation and interaction, refining data collection, a full open-source release, and adding new games and informational material about research on the senses.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="luca_euromedia_2022" class="col-12"> <div class="title">Exploring the Dimensionality of the Affective Space Elicited by Gendered Toy Commercials</div> <div class="author"> Luca Marinelli, and Charalampos Saitis</div> <div class="periodical"> <em>9th European Conference on Media, Communication &amp; Film</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>As evidenced by a large body of literature, the gender-stereotyped nature of toy adverts has been widely scrutinised. However, little work has been done in examining the affective impact of these commercials on the audience. It has been proven that repeated exposure to gender-stereotyped messages has the capacity to influence behaviours, beliefs and attitudes. In particular, media can influence emotion socialization, and gender differences in emotion expression might emerge (Scherr 2018). In this study, we investigated whether commercials elicit emotions at different intensities with respect to the gender of their target audience. Furthermore, we evaluated whether such emotions follow distinct underlying latent structures. A total of 1081 ratings of 10 unipolar aesthetic emotion scales were collected for 135 commercials (45 for each masculine, feminine, and mixed target audience) from 80 UK nationals (35 F, 45 M) aged 18 to 76. The main reason for collecting our ratings from adults was that, already by age 11, children exhibit adult-like emotion recognition capabilities (Hunter 2011). Seven scales showed significant differences between commercials for distinct audiences; with five, in particular, revealing a strong polarization (happiness, amusement, beauty, calm, and anger). In addition, parallel analysis showed that a minimum of three factors are needed to explain the ratings for masculine and mixed targeted commercials, while only two are needed for the feminine ones, thereby indicating that the latter elicit emotions following a simpler underlying structure. Both results reflect larger issues in toy marketing, where gender essentialism is still dominant, and prompt further discussion and research.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="russell_ijcnn_2022" class="col-12"> <div class="title">Timbre Transfer with Variational Auto Encoding and Cycle-Consistent Adversarial Networks</div> <div class="author"> Russell Sammut Bonnici, Martin Benning, and Charalampos Saitis</div> <div class="periodical"> <em>International Joint Conference on Neural Networks</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/paper_IJCNN_2022.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/poster_IJCNN_2022.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>This work investigates the application of deep learning to timbre transfer. The adopted approach combines Variational Autoencoders with Generative Adversarial Networks to construct meaningful representations of the source audio and produce realistic generations of the target audio and is applied to the Flickr 8k Audio dataset for transferring the vocal timbre between speakers and the URMP dataset for transferring the musical timbre between instruments. Variations of the adopted approach were trained, and performance was compared using the metrics SSIM (Structural Similarity Index) and FAD (Frechet Audio Distance). It was found that a many-to-many approach supersedes a one-to-one approach in terms of reconstructive capabilities, while one-to-one showed better results in terms of adversarial translation. The adoption of a basic over a bottleneck residual block design is more suitable for enriching content information about a latent space, and the decision on whether cyclic loss takes on a variational autoencoder or vanilla autoencoder approach does not have a significant impact on reconstructive and adversarial translation aspects of the model.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="hayes_disembodied_2022" class="col-12"> <div class="title">Disembodied Timbres: A Study on Semantically Prompted FM Synthesis</div> <div class="author"> Ben Hayes, Charalampos Saitis, and György Fazekas</div> <div class="periodical"> <em>Journal of the Audio Engineering Society</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/hayes_disembodied_2022.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Disembodied electronic sounds constitute a large part of the modern auditory lexicon, but research into timbre perception has focused mostly on the tones of conventional acoustic musical instruments. It is unclear whether insights from these studies generalise to electronic sounds, nor is it obvious how these relate to the creation of such sounds. In this work, we present an experiment on the semantic associations of sounds produced by FM synthesis with the aim of identifying whether existing models of timbre semantics are appropriate for such sounds. We applied a novel experimental paradigm in which experienced sound designers responded to semantic prompts by programming a synthesiser, and provided semantic ratings on the sounds they created. Exploratory factor analysis revealed a five-dimensional semantic space. The first two factors mapped well to the concepts of luminance, texture, and mass. The remaining three factors did not have clear parallels, but correlation analysis with acoustic descriptors suggested an acoustical relationship to luminance and texture. Our results suggest that further enquiry into the timbres of disembodied electronic sounds, their synthesis, and their semantic associations would be worthwhile, and that this could benefit research into auditory perception and cognition, as well as synthesis control and audio engineering.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="Alejandro_SMC_2022" class="col-12"> <div class="title">Deep Embeddings for Robust User-Based Amateur Vocal Percussion Transcription</div> <div class="author"> Alejandro Delgado, Emir Demirel, Vinod Subramanian, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Charalampos Saitis, Mark Sandler' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>19th Sound and Music Computing Conference</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2204.04646" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://zenodo.org/record/6797666#.YyhGDuxKjOQ" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Vocal Percussion Transcription (VPT) is concerned with the automatic detection and classification of vocal percussion sound events, allowing music creators and producers to sketch drum lines on the fly among others. VPT classifiers usually learn best from small user-specific datasets, which usually restrict modelling to small input feature sets to avoid model overfitting. This study explores several deep supervised learning strategies to obtain informative feature sets for amateur VPT classification. We evaluated their performance on regular VPT classification tasks and compared them with several baseline approaches including feature selection methods and a state-of-the-art speech recognition engine. These proposed learning models were supervised with several label sets containing information from four different levels of abstraction: instrument-level, syllable-level, phoneme-level, and boxeme-level. Results suggest that convolutional neural networks supervised with syllable-level annotations produced the most informative embeddings for VPT systems, which can be used as input representations to fit classifiers with. Finally, we used back-propagation-based saliency maps to investigate the importance of difference spectrogram regions for feature learning.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="Saitis_SMPC_22" class="col-12"> <div class="title">Auditory brightness perception investigated by unimodal and crossmodal interference</div> <div class="author"> Charalampos Saitis, Zachary Wallmark, and Annie Liu</div> <div class="periodical"> <em>Biennial Meeting of the Society for Music Perception and Cognition</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Saitis_SMPC_22.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>Brightness is among the most studied aspects of timbre perception. Psychoacoustically, sounds described as ”bright” vs ”dark” typically exhibit a high vs low frequency emphasis in the spectrum. However, relatively little is known about the neurocognitive mechanisms that facilitate these “metaphors we listen with.” Do they originate in universal mental representations common to more than one sensory modality? Triangulating three different interaction paradigms, we investigated using speeded identification whether unimodal and crossmodal interference occurs when timbral brightness, as modelled by the centroid of the spectral envelope, and 1) pitch height, 2) visual brightness, 3) numerical value processing are semantically incongruent. In three online pilot tasks, 58 participants were presented a baseline stimulus (a pitch, gray square, or numeral) then asked to quickly identify a target stimulus that is higher/lower, brighter/darker, or greater/less than the baseline, respectively, after being primed with a bright or dark synthetic harmonic tone. Additionally, in the pitch and visual tasks, a deceptive same-target condition was included. Results suggest that timbral brightness modulates the perception of pitch and visual brightness, but not numerical value. Semantically incongruent pitch height-timbral brightness shifts produced significantly slower choice reaction time and higher error compared to congruent pairs; timbral brightness also had a strong biasing effect in the same-target condition (i.e., people heard the same pitch as higher when the target tone was timbrally brighter than the baseline, and vice versa with darker tones). In the visual task, incongruent pairings of gray squares and tones elicited slower choice reaction times than congruent pairings. No interference was observed in the number comparison task. We are currently following up on these results with a larger online replication sample, and an fMRI study to investigate the relevant neural mechanisms. Our findings shed light on the multisensory nature of experiencing timbre.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="haid_2022_proc" class="col-12"> <div class="title">Proceedings of the 11th International Workshop on Haptic and Audio Interaction Design</div> <div class="author"> Eds: Charalampos Saitis, Ildar Farkhatdinov, and Stefano Papetti</div> <div class="periodical"> Springer Lecture Notes in Computer Science 13417, 2022 </div> <div class="links"> <a href="https://link.springer.com/book/10.1007/978-3-031-15019-7" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Link</a> </div> </div> </div> </li> </ol> <h4 class="year">2021</h4> <ol class="bibliography"> <li> <div class="row"> <div id="actor_icmpc_2021" class="col-12"> <div class="title">Mapping the semantics of timbre across pitch registers</div> <div class="author"> Lindsey Reymore, Jason Noble, Charalampos Saitis, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Caroline Traube, Zachary Wallmark' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>16th International Conference on Music Perception and Cognition</em>, 2021 </div> <div class="links"> <a href="https://drive.google.com/file/d/1oax1QfdTgmP5bWZF7aOoNyLbm5qFQsex/view" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Abs</a> <a href="https://www.youtube.com/watch?v=c4xJSSB5QVs" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> </div> <div class="abstract hidden"> <p>https://drive.google.com/file/d/1oax1QfdTgmP5bWZF7aOoNyLbm5qFQsex/view</p> </div> </div> </div> </li> <li> <div class="row"> <div id="saitis2021multimodal" class="col-12"> <div class="title">Multimodal Classification of Stressful Environments in Visually Impaired Mobility Using EEG and Peripheral Biosignals</div> <div class="author"> Charalampos Saitis, and Kyriaki Kalimeri</div> <div class="periodical"> <em>IEEE Transactions on Affective Computing</em>, 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/59824/Saitis%20Multimodal%20Classification%20of%20Stressful%202019%20Accepted.pdf?sequence=2" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>In this study, we aim to better understand the cognitive-emotional experience of visually impaired people when navigating in unfamiliar urban environments, both outdoor and indoor. We propose a multimodal framework based on random forest classifiers, which predict the actual environment among predefined generic classes of urban settings, inferring on real-time, non-invasive, ambulatory monitoring of brain and peripheral biosignals. Model performance reached 93% for the outdoor and 87% for the indoor environments (expressed in weighted AUROC), demonstrating the potential of the approach. Estimating the density distributions of the most predictive biomarkers, we present a series of geographic and temporal visualizations depicting the environmental contexts in which the most intense affective and cognitive reactions take place. A linear mixed model analysis revealed significant differences between categories of vision impairment, but not between normal and impaired vision. Despite the limited size of our cohort, these findings pave the way to emotionally intelligent mobility-enhancing systems, capable of implicit adaptation not only to changing environments but also to shifts in the affective state of the user in relation to different environmental and situational factors.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="vjosa_cmmr_2021" class="col-12"> <div class="title">Modelling Moral Traits with Music Listening Preferences and Demographics</div> <div class="author"> Vjosa Preniqi, Kyriaki Kalimeri, and Charalampos Saitis</div> <div class="periodical"> <em>15th International Symposium on Computer Music Multidisciplinary Research</em>, 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://cmmr2021.github.io/proceedings/pdffiles/cmmr2021_08.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Music has always been an integral part of our everyday lives through which we express feelings, emotions, and concepts. Here, we explore the association between music genres, demographics and moral values employing data from an ad-hoc online survey and the Music Learning Histories Dataset. To further characterise the music preferences of the participants the generalist/specialist (GS) score employed. We exploit both classification and regression approaches to assess the predictive power of music preferences for the prediction of demographic attributes as well as the moral values of the participants. Our findings point out that moral values are hard to predict (.62 AUROC_avg) solely by the music listening behaviours, while if basic sociodemographic information is provided the prediction score rises to 4% on average (.66 AUROC_avg), with the Purity foundation to be the one that is steadily the one with the highest accuracy scores. Similar results are obtained from the regression analysis. Finally, we provide with insights on the most predictive music behaviours associated with each moral value that can inform a wide range of applications from rehabilitation practices to communication campaign design.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="charis_sempre_2021" class="col-12"> <div class="title">Development of a Web Application for the Education, Assessment, and Study of Timbre Perception</div> <div class="author"> Charalampos Saitis</div> <div class="periodical"> <em>Society for Education, Music, and Psychology Research Conference</em>, 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Timbre is defined as any auditory property other than pitch, duration, and loudness that allows two sounds to be distinguished. The Timbre Explorer (TE) is a synthesiser interface designed to demonstrate timbral dimensions of sound. This project aimed to develop and evaluate a web version of the TE that attempts to train its users and test their understanding of timbre as they go through a series of gamified tasks. A pilot study with 16 participants helped to identify shortcomings ahead of a full-sized study that will evaluate the performance of the TE as an educational aid and musical assessment tool.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="vjosa_ic2s2_2021" class="col-12"> <div class="title">We are what we listen to: How moral values reflect on musical preferences</div> <div class="author"> Vjosa Preniqi, Kyriaki Kalimeri, and Charalampos Saitis</div> <div class="periodical"> <em>7th International Conference on Computational Social Science</em>, 2021 </div> <div class="links"> <a href="/assets/pdf/IC2S2_Poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> </div> </div> </li> <li> <div class="row"> <div id="hayes_neural_2021" class="col-12"> <div class="title">Neural Waveshaping Synthesis</div> <div class="author"> Ben Hayes, Charalampos Saitis, and György Fazekas</div> <div class="periodical"> <em>22nd International Society for Music Information Retrieval Conference</em>, 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/hayes_neural_2021.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We present the Neural Waveshaping Unit (NEWT): a novel, lightweight, fully causal approach to neural audio synthesis which operates directly in the waveform domain, with an accompanying optimisation (FastNEWT) for efficient CPU inference. The NEWT uses time-distributed multilayer perceptrons with periodic activations to implicitly learn nonlinear transfer functions that encode the characteristics of a target timbre. Once trained, a NEWT can produce complex timbral evolutions by simple affine transformations of its input and output signals. We paired the NEWT with a differentiable noise synthesiser and reverb and found it capable of generating realistic musical instrument performances with only 260k total model parameters, conditioned on F0 and loudness features. We compared our method to state-of-the-art benchmarks with a multi-stimulus listening test and the Fréchet Audio Distance and found it performed competitively across the tested timbral domains. Our method significantly outperformed the benchmarks in terms of generation speed, and achieved real-time performance on a consumer CPU, both with and without FastNEWT, suggesting it is a viable basis for future creative sound design tools.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="hayes_perceptual_2021" class="col-12"> <div class="title">Perceptual and semantic scaling of FM synthesis timbres: Common dimensions and the role of expertise</div> <div class="author"> Ben Hayes, Charalampos Saitis, and György Fazekas</div> <div class="periodical"> <em>16th International Conference on Music Perception and Cognition</em>, 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/hayes_perceptual_2021.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Electronic sound has a rich history, yet timbre research has typically focused on the sounds of physical instruments, while synthesised sound is often relegated to functional roles like recreating acoustic timbres. Studying the perception of synthesised sound can broaden our conception of timbre and improve musical synthesis tools. We aimed to identify the perceptually salient acoustic attributes of sounds produced by frequency modulation synthesis. We also aimed to test Zacharakis et al’s luminance-texture-mass timbre semantic model [Music Perception, 31, 339–358 (2014)] in this domain. Finally, we aimed to identify effects of prior music or synthesis experience on these results. Our results suggest that discrimination of abstract electronic timbres may rely on attributes distinct from those used with acoustic timbres. Further, the most salient attributes vary with expertise. However, the use of semantic descriptors is similar to that of acoustic instruments, and is consistent across expertise levels.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="hayes_nash_2021" class="col-12"> <div class="title">NASH: the Neural Audio Synthesis Hackathon</div> <div class="author"> Ben Hayes, Cyrus Vahidi, and Charalampos Saitis</div> <div class="periodical"> <em>DMRN+16: Digital Music Research Network One-Day Workshop</em>, 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/ben-DMRN-16.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The field of neural audio synthesis aims to produce audio using neural networks. A recent surge in its popularity has led to several high profile works achieving impressive feats of speech and music synthesis. The development of broadly accessible neural audio synthesis tools, conversely, has been limited, and creative applications of these technologies are mostly undertaken by those with technical know-how. Research has focused largely on tasks such as realistic speech and musical instrument synthesis, whereas investigations into high-level control, esoteric sound design capabilities, and interpretability have received less attention. To encourage innovative work addressing these gaps, C4DM’s Special Interest Group on Neural Audio Synthesis (SIGNAS) propose to host our first Neural Audio Synthesis Hackathon: a two day event, with results to be presented in a session at DMRN+16.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="cyrus_dmrn_2021" class="col-12"> <div class="title">Acoustic Representations for Perceptual Timbre Similarity</div> <div class="author"> Cyrus Vahidi, Ben Hayes, Charalampos Saitis, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? ' György' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>DMRN+16: Digital Music Research Network One-Day Workshop</em>, 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/cyrus-DMRN-16.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In this work, we outline initial steps towards modelling perceptual timbre dissimilarity. We use stimuli from 17 distinct subjective timbre studies and compute pairwise distances in the spaces of MFCCs, joint time-frequency scattering coefficients and Open-L3 embeddings. We analyze agreement of distances in these spaces with human dissimilarity ratings and highlight challenges of this task.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="russell_dmrn_2021" class="col-12"> <div class="title">Variational Auto Encoding and Cycle-Consistent Adversarial Networks for Timbre Transfer</div> <div class="author"> Russell Sammut Bonnici, Martin Benning, and Charalampos Saitis</div> <div class="periodical"> <em>DMRN+16: Digital Music Research Network One-Day Workshop</em>, 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/russell-DMRN-16.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The combination of Variational Autoencoders (VAE) with Generative Adversarial Networks (GAN) motivates meaningful representations of audio in the context of timbre transfer. This was applied to different datasets for transferring vocal timbre between speakers and musical timbre between instruments. Variations of the approach were trained and generalised performance was compared using the Structural Similarity Index and Frechet Audio Distance. Many-to-many style transfer was found to improve reconstructive performance over one-to-one style transfer.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="IJCNN_2021_Accepted" class="col-12"> <div class="title">A Modulation Front-End for Music Audio Tagging</div> <div class="author"> Cyrus Vahidi, Charalampos Saitis, and György Fazekas</div> <div class="periodical"> <em>International Joint Conference on Neural Networks</em>, 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/IJCNN_2021_Accepted.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Convolutional Neural Networks have been extensively explored in the task of automatic music tagging. The problem can be approached by using either engineered time-frequency features or raw audio as input. Modulation filter bank representations that have been actively researched as a basis for timbre perception have the potential to facilitate the extraction of perceptually salient features. We explore end-to-end learned front-ends for audio representation learning, ModNet and SincModNet, that incorporate a temporal modulation processing block. The structure is effectively analogous to a modulation filter bank, where the FIR filter center frequencies are learned in a data-driven manner. The expectation is that a perceptually motivated filter bank can provide a useful representation for identifying music features. Our experimental results provide a fully visualisable and interpretable front-end temporal modulation decomposition of raw audio. We evaluate the performance of our model against the state-of-the-art of music tagging on the MagnaTagATune dataset. We analyse the impact on performance for particular tags when time-frequency bands are subsampled by the modulation filters at a progressively reduced rate. We demonstrate that modulation filtering provides promising results for music tagging and feature representation, without using extensive musical domain knowledge in the design of this frontend.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="Alejandro_AES151" class="col-12"> <div class="title">Phoneme Mappings for Online Vocal Percussion Transcription</div> <div class="author"> Alejandro Delgado, Charalampos Saitis, and Mark Sandler</div> <div class="periodical"> <em>151st Audio Engineering Society Convention</em>, 2021, Honourable Mention for Outstanding Paper </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Alejandro_AES151.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Vocal Percussion Transcription (VPT) aims at detecting vocal percussion sound events in a beatboxing performance and classifying them into the correct drum instrument class (kick, snare, or hi-hat). To do this in an online (real-time) setting, however, algorithms are forced to classify these events within just a few milliseconds after they are detected. The purpose of this study was to investigate which phoneme-to-instrument mappings are the most robust for online transcription purposes. We used three different evaluation criteria to base our decision upon: frequency of use of phonemes among different performers, spectral similarity to reference drum sounds, and classification separability. With these criteria applied, the recommended mappings would potentially feel natural for performers to articulate while enabling the classification algorithms to achieve the best performance possible. Given the final results, we provided a detailed discussion on which phonemes to choose given different contexts and applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="Alejandro_ICMC_2021" class="col-12"> <div class="title">Learning Models for Query by Vocal Percussion: A Comparative Study</div> <div class="author"> Alejandro Delgado, McDonald SKoT, Ning Xu, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Charalampos Saitis, Mark Sandler' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>46th International Computer Music Conference</em>, 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2110.09223" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> <div class="abstract hidden"> <p>The imitation of percussive sounds via the human voice is a natural and effective tool for communicating rhythmic ideas on the fly. Thus, the automatic retrieval of drum sounds using vocal percussion can help artists prototype drum patterns in a comfortable and quick way, smoothing the creative workflow as a result. Here we explore different strategies to perform this type of query, making use of both traditional machine learning algorithms and recent deep learning techniques. The main hyperparameters from the models involved are carefully selected by feeding performance metrics to a grid search algorithm. We also look into several audio data augmentation techniques, which can potentially regularise deep learning models and improve generalisation. We compare the final performances in terms of effectiveness (classification accuracy), efficiency (computational speed), stability (performance consistency), and interpretability (decision patterns), and discuss the relevance of these results when it comes to the design of successful query-by-vocal-percussion systems.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="Lam_NIME21" class="col-12"> <div class="title">The Timbre Explorer: A Synthesizer Interface for Educational Purposes and Perceptual Studies</div> <div class="author"> Joshua Ryan Lam, and Charalampos Saitis</div> <div class="periodical"> <em>International Conference on New Interfaces for Musical Expression</em>, 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.21428/92fbeb44.92a95683" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Link</a> </div> <div class="abstract hidden"> <p>When two sounds are played at the same loudness, pitch, and duration, what sets them apart are their timbres. This study documents the design and implementation of the Timbre Explorer, a synthesizer interface based on efforts to dimensionalize this perceptual concept. The resulting prototype controls four perceptually salient dimensions of timbre in real-time: attack time, brightness, spectral flux, and spectral density. A graphical user interface supports user understanding with live visualizations of the effects of each dimension. The applications of this interface are three-fold; further perceptual timbre studies, usage as a practical shortcut for synthesizers, and educating users about the frequency domain, sound synthesis, and the concept of timbre. The project has since been expanded to a standalone version independent of a computer and a purely online web-audio version.</p> </div> </div> </div> </li> </ol> <h4 class="year">2020</h4> <ol class="bibliography"> <li> <div class="row"> <div id="ben_sound_instruments_2020" class="col-12"> <div class="title">How we talk about sound: Semantic dimensions of abstract timbres</div> <div class="author"> Ben Hayes, and Charalampos Saitis</div> <div class="periodical"> <em>Sound Instruments and Sonic Cultures: An Interdisciplinary Conference</em>, 2020, National Science &amp; Media Museum </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Synthesisers, in their many forms, enable the realisation of almost any conceivable sound. Their fine-grained control and broad timbral palette call for a descriptive lexicon to enable their verbal differentiation and discussion. While acoustic instruments of the western classical lineage are the subject of an extensive body of enquiry into the perceptual attributes and semantic associations of the sounds they produce, abstract electronic sounds have been comparatively understudied in this regard. In particular, the diverse vocabulary used to describe such classical acoustic instruments can be summarised with three conceptual metaphors—such musical tones have luminance, texture, and mass—but this has yet to be explicitly confirmed for the kinds of electronic sounds that pervade many modern sonic cultures. In this work, we present an experimental paradigm for studying the semantic associations of synthesised sounds, wherein a group of experienced music producers and sound designers interacted with a web-based synthesiser in response to descriptive prompts, and provided comparative semantic ratings on the sounds they created. The words used for semantic ratings were selected by mining a text corpus from the popular modular synthesis forum Muff Wiggler, and analysing the frequency of adjectives in contexts pertaining to timbre. The ratings provided by participants were subject to statistical analysis. From 27 initial adjectives, two underlying semantic factors were revealed: terms including aggressive, hard, and complex associated with the first, and dark and warm with the second. These factors differ from those found for classical acoustic sounds, implying a relationship between the qualia of a sonic experience and the language employed to talk about it. Such insight has implications for how sound is conceptualised, understood, and received within sonic cultures—in particular, those predicated on electronic or abstract sound—and applications in developing novel control schemes for synthesis methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="max_eurohaptics_2020" class="col-12"> <div class="title">Analysing and countering bodily interference in vibrotactile devices introduced by human interaction and physiology</div> <div class="author"> Maximilian Weber, and Charalampos Saitis</div> <div class="periodical"> <em>12th EuroHaptics Conference</em>, 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p></p> </div> </div> </div> </li> <li> <div class="row"> <div id="saitis2020timbre" class="col-12"> <div class="title">Timbre semantics through the lens of crossmodal correspondences: A new way of asking old questions</div> <div class="author"> Charalampos Saitis, Stefan Weinzierl, Katharina Kriegstein, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Sølvi Ystad, Christine Cuskley' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Acoustical Science and Technology</em>, 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/charis_AST_2020.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This position paper argues that a systematic study of the behavioral and neural mechanisms of crossmodal correspondences between timbral dimensions of sound and perceptual dimensions of other sensory modalities, such as brightness, roughness, or sweetness, can offer a new way of addressing old questions about the perceptual and neurocognitive mechanisms of auditory semantics. At the same time, timbre and the crossmodal metaphors that dominate its conceptualization can provide a test case for better understanding the neural basis of crossmodal correspondences and human semantic processing in general.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="senses_survey" class="col-12"> <div class="title">What do people know about sensation and perception? Understanding perceptions of sensory experience</div> <div class="author"> Christine Cuskley, and Charalampos Saitis</div> <div class="periodical"> <em>PsyArXiv</em>, 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://psyarxiv.com/ghcxv" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PsyArXiv</a> </div> <div class="abstract hidden"> <p>Academic disciplines spanning cognitive science, art, and music have made strides in understanding how humans sense and experience the world. We now have a better scientific understanding of how human sensation and perception function both in the brain and in interaction than ever before. However, there is little research on how this high level scientific understanding is translated into knowledge for the public more widely. We present descriptive results from a simple survey and compare how public understanding and perception of sensory experience lines up with scientific understanding. Results show that even in a sample with fairly high educational attainment, many respondents were unaware of fairly common forms of sensory variation. In line with the well-documented under representation of sign languages within linguistics, respondents tended to under-estimate the number of sign languages in the world. We outline how our results represent gaps in public understanding of sensory variation, and argue that filling these gaps can form an important early intervention, acting as a basic foundation for improving acceptance, inclusivity, and accessibility for cognitively diverse populations.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="marentakis:hal-03234063" class="col-12"> <div class="title">Timbre in Binaural Listening: A Comparison of Timbre Descriptors in Anechoic and HRTF Filtered Orchestral Sounds</div> <div class="author"> Georgios Marentakis, and Charalampos Saitis</div> <div class="periodical"> <em>Forum Acusticum</em>, 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://hal.archives-ouvertes.fr/hal-03234063/file/000992.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>The psychoacoustic investigation of timbre traditionally relies on audio descriptors extracted from anechoic or semi-anechoic recordings of musical instrument sounds, which are presented to listeners in diotic fashion. As a result, the extent to which spectral modifications due to the outer ear interact with timbre perception is not fully understood. As a first step towards investigating this research question, we examine here whether timbre descriptors calculated using HRTF filtered instrumental sounds deviate across ears and from values obtained from the same sounds without HRTF filtering for different listeners. The sound set comprised isolated notes played at the same fundamental frequency and dynamic from a database of anechoic recordings of modern orchestral instruments and some of their classical and baroque precursors. These were convolved with anechoic high spatial resolution HRTFs of human listeners. We present results and discuss implications for research on timbre perception and cognition.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="hayes_perceptual_2020" class="col-12"> <div class="title">Perceptual Similarities in Neural Timbre Embeddings</div> <div class="author"> Ben Hayes, Luke Brosnahan, Charalampos Saitis, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'György Fazekas' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>DMRN+15: Digital Music Research Network One-Day Workshop</em>, 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/hayes_perceptual_2020.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Many neural audio synthesis models learn a representational space which can be used for control or exploration of the sounds generated. It is unclear what relationship exists between this space and human perception of these sounds. In this work, we compute configurational similarity metrics between an embedding space learned by a neural audio synthesis model and conventional perceptual and seman- tic timbre spaces. These spaces are computed using abstract synthesised sounds. We find significant similarities between these spaces, suggesting a shared organisational influence.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="hayes_theres_2020" class="col-12"> <div class="title">There’s More to Timbre than Musical Instruments: Semantic Dimensions of FM Sounds</div> <div class="author"> Ben Hayes, and Charalampos Saitis</div> <div class="periodical"> <em>2nd International Conference on Timbre</em>, 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/hayes_theres_2020.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Much previous research into timbre semantics (such as when an oboe is described as “hollow”) has focused on sounds produced by acoustic instruments, particularly those associated with western tonal music (Saitis &amp; Weinzierl, 2019). Many synthesisers are capable of producing sounds outside the timbral range of physical instruments, but which are still discriminable by their timbre. Research into the perception of such sounds, therefore, may help elucidate further the mechanisms underpinning our experience of timbre in the broader sense. In this paper, we present a novel paradigm on the application of semantic descriptors to sounds produced by experienced sound designers using an FM synthesiser with a full set of controls.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="zacharakis_evidence_2020" class="col-12"> <div class="title">Evidence for Timbre Space Robustness to an Uncontrolled Online Stimulus Presentation</div> <div class="author"> Asterios Zacharakis, Ben Hayes, Charalampos Saitis, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Konstantinos Pastiadis' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>2nd International Conference on Timbre</em>, 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/zacharakis_evidence_2020.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Research on timbre perception is typically conducted under controlled laboratory conditions where every effort is made to maintain stimulus presentation conditions fixed (McAdams, 2019). This conforms with the ANSI (1973) definition of timbre suggesting that in order to judge the timbre differences between a pair of sounds the rest perceptual attributes (i.e., pitch, duration and loudness) should remain unchanged. Therefore, especially in pairwise dissimilarity studies, particular care is taken to ensure that loudness is not used by participants as a criterion for judgements by equalising it across experimental stimuli. On the other hand, conducting online experiments is an increasingly favoured practice in the music perception and cognition field as targeting relevant communities can potentially provide a large number of suitable participants with relatively little time investment from the side of the experimenters (e.g., Woods et al., 2015). However, the strict requirements for stimuli preparation and presentation prevents timbre studies from conducting online experimentation. Despite the obvious difficulties in imposing equal loudness on online experiments, the different playback equipment chain (DACs, pre-amplifiers, headphones) will also almost inevitably ‘colour’ the sonic outcome in a different way. Despite the above limitations, in a social distancing time like this, it would be of major importance to be able to lift some of the physical requirements in order to carry on conducting behavioural research on timbre perception. Therefore, this study aims to investigate the extent to which an uncontrolled online replication of a past laboratory-conducted pairwise dissimilarity task will distort the findings.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="delgado_vocimit_2020" class="col-12"> <div class="title">Spectral and Temporal Timbral Cues of Vocal Imitations</div> <div class="author"> Alejandro Delgado, Charalampos Saitis, and Mark Sandler</div> <div class="periodical"> <em>2nd International Conference on Timbre</em>, 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/delgado_timbre_2020.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The imitation of non-vocal sounds using the human voice is a resource we sometimes rely on when communicating sound concepts to other people. Query by Vocal Percussion (QVP) is a subfield in Music Information Retrieval (MIR) that explores techniques to query percussive sounds using vocal imitations as input, usually plosive consonant sounds. The goal of this work was to investigate timbral relationships between real drum sounds and their vocal imitations. We believe these insights could shed light on how to select timbre descriptors for extraction when designing offline and online QVP systems. In particular, we studied a dataset composed of 30 acoustic and electronic drum sound recordings and vocal imitations of each sound performed by 14 musicians. Our approach was to study the correlation of audio content descriptors of timbre extracted from the drum samples with the same descriptors taken from vocal imitations. Three timbral descriptors were selected: the Log Attack Time (LAT), the Spectral Centroid (SC), and the Derivative After Maximum of the sound envelope (DAM). LAT and SC have been shown to represent salient dimensions of timbre across different types of sounds including percussion. In this sense, one intriguing question would be to what extent listeners can communicate these salient timbral cues in vocal imitations. The third descriptor, DAM, was selected for its role in describing the sound’s tail, which we considered to be a relevant part of percussive utterances.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="vahidi_timbre_2020" class="col-12"> <div class="title">Timbre Space Representation of a Subtractive Synthesizer</div> <div class="author"> Cyrus Vahidi, György Fazekas, Charalampos Saitis, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Alessandro Palladini' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>2nd International Conference on Timbre</em>, 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2009.11706.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>In this study, we produce a geometrically scaled perceptual timbre space from dissimilarity ratings of subtractive synthesized sounds and correlate the resulting dimensions with a set of acoustic descriptors. We curate a set of 15 sounds, produced by a synthesis model that uses varying source waveforms, frequency modulation (FM) and a lowpass filter with an enveloped cutoff frequency. Pairwise dissimilarity ratings were collected within an online browser-based experiment. We hypothesized that a varied waveform input source and enveloped filter would act as the main vehicles for timbral variation, providing novel acoustic correlates for the perception of synthesized timbres.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="drouzas_timbre_2020" class="col-12"> <div class="title">Verbal description of musical brightness</div> <div class="author"> Christos Drouzas, and Charalampos Saitis</div> <div class="periodical"> <em>2nd International Conference on Timbre</em>, 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://timbre2020.mus.auth.gr/assets/papers/9.Drouzas.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Amongst the most common descriptive expressions of timbre used by musicians, music engineers, audio researchers as well as everyday listeners are words related to the notion of brightness (e.g., bright, dark, dull, brilliant, shining). From a psychoacoustic perspective, brightness ratings of instrumental timbres as well as music excerpts systematically correlate with the centre of gravity of the spectral envelope and thus brightness as a semantic descriptor of musical sound has come to denote a prevalence of high-frequency over low-frequency energy. However, relatively little is known about the higher-level cognitive processes underpinning musical brightness ratings. Psycholinguistic investigations of verbal descriptions of timbre suggest a more complex, polysemic picture (Saitis &amp; Weinzierl 2019) that warrants further research. To better understand how musical brightness is conceptualised by listeners, here we analysed free verbal descriptions collected along brightness ratings of short music snippets (involving 69 listeners) and brightness ratings of orchestral instrument notes (involving 68 listeners). Such knowledge can help delineate the intrinsic structure of brightness as a perceptual attribute of musical sounds, and has broad implications and applications in orchestration, audio engineering, and music psychology.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="charis_JASA_2020" class="col-12"> <div class="title">Brightness perception for musical instrument sounds: Relation to timbre dissimilarity and source-cause categories</div> <div class="author"> Charalampos Saitis, and Kai Siedenburg</div> <div class="periodical"> <em>The Journal of the Acoustical Society of America</em>, 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/2256_1_final_published.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Timbre dissimilarity of orchestral sounds is well-known to be multidimensional, with attack time and spectral centroid representing its two most robust acoustical correlates. The centroid dimension is traditionally considered as reflecting timbral brightness. However, the question of whether multiple continuous acoustical and/or categorical cues influence brightness perception has not been addressed comprehensively. A triangulation approach was used to examine the dimensionality of timbral brightness, its robustness across different psychoacoustical contexts, and relation to perception of the sounds’ source-cause. Listeners compared 14 acoustic instrument sounds in three distinct tasks that collected general dissimilarity, brightness dissimilarity, and direct multi-stimulus brightness ratings. Results confirmed that brightness is a robust unitary auditory dimension, with direct ratings recovering the centroid dimension of general dissimilarity. When a two-dimensional space of brightness dissimilarity was considered, its second dimension correlated with the attack-time dimension of general dissimilarity, which was interpreted as reflecting a potential infiltration of the latter into brightness dissimilarity. Dissimilarity data were further modeled using partial least-squares regression with audio descriptors as predictors. Adding predictors derived from instrument family and the type of resonator and excitation did not improve the model fit, indicating that brightness perception is underpinned primarily by acoustical rather than source-cause cues.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="weber2020towards" class="col-12"> <div class="title">Towards a framework for ubiquitous audio-tactile design</div> <div class="author"> Maximilian Weber, and Charalampos Saitis</div> <div class="periodical"> <em>10th International Workshop on Haptic and Audio Interaction Design</em>, 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://hal.archives-ouvertes.fr/hal-02901209/document" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>To enable a transition towards rich vibrotactile feedback in applications and media content, a complete end-to-end system — from the design of the tactile experience all the way to the tactile stimulus reproduction — needs to be considered. Currently, most applications are at best limited to dull vibration patterns due to limited hard- and software implementations, while the design of ubiquitous platform-agnostic tactile stimuli remains challenging due to a lack of standardized protocols and tools for tactile design, storage, transport, and reproduction. This work proposes a conceptual framework, utilizing audio assets as a starting point for the design of vibrotactile stimuli, including ideas for a parametric tactile data model, and outlines challenges for a platform-agnostic stimuli reproduction. Finally, the benefits and shortcomings of a commercial and wide-spread vibrotactile API are investigated as an example for the current state of a complete end-to-end framework.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="luca_SMC_2020" class="col-12"> <div class="title">Musical dynamics classification with CNN and modulation spectra</div> <div class="author"> Luca Marinelli, Athanasios Lykartsis, Stefan Weinzierl, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Charalampos Saitis' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>17th Sound and Music Computing Conference</em>, 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.researchgate.net/profile/Charalampos-Saitis/publication/343775001_MUSICAL_DYNAMICS_CLASSIFICATION_WITH_CNN_AND_MODULATION_SPECTRA/links/5f3ee47392851cd3020dac6e/MUSICAL-DYNAMICS-CLASSIFICATION-WITH-CNN-AND-MODULATION-SPECTRA.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>To investigate variations in the timbre space with regards to musical dynamics, convolutional neural networks (CNNs) were trained on modulation power spectra (MPS), melscaled and ERB-scaled spectrograms of single notes of sustained instruments played at two dynamics extremes (pp and ff). The samples, from an extensive dataset of several timbre families, were rms normalized in order to eliminate the loudness information and force the network to focus on timbre attributes of musical dynamics that are shared across different instrument families. The proposed CNN architecture obtained competitive results in three classification tasks with all three input representations. In order to compare the different input representations, the test sets in three experiments were partitioned in order to promote or avoid selection bias. When selection bias was avoided, models trained on MPS were outperformed by those trained on time-frequency representations, conversely, those trained on MPS achieved the best results when selection bias was promoted. Low-temporal modulations emerged in classspecific MPS saliency maps as markers of musical dynamics. This led to the implementation of a MPS-based scalar descriptor of timbre that largely outperformed the chosen baseline (44.8% error reduction).</p> </div> </div> </div> </li> <li> <div class="row"> <div id="Timbre_2020_proc" class="col-12"> <div class="title">Proceedings of the 2nd International Conference on Timbre</div> <div class="author"> Eds: Asterios Zacharakis, Charalampos Saitis, and Kai Siedenburg</div> <div class="periodical"> The School of Music Studies, Aristotle University of Thessaloniki, 2020 </div> <div class="links"> <a href="http://timbre2020.mus.auth.gr/assets/papers/Timbre2020_proceedings.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Link</a> </div> </div> </div> </li> </ol> <h4 class="year">2019</h4> <ol class="bibliography"> <li> <div class="row"> <div id="luca_dmrn_2019" class="col-12"> <div class="title">Modulation Spectra for Musical Dynamics Perception and Retrieval</div> <div class="author"> Luca Marinelli, Athanasios Lykartsis, and Charalampos Saitis</div> <div class="periodical"> <em>DMRN+14: Digital Music Research Network One-Day Workshop</em>, 2019 </div> <div class="links"> <a href="/assets/pdf/luca_dmrn_2019.pdf" class="btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>luca_dmrn_2019.pdf</p> </div> </div> </div> </li> <li> <div class="row"> <div id="charis_ica_2019" class="col-12"> <div class="title">The role of attack transients in timbral brightness perception</div> <div class="author"> Charalampos Saitis, Kai Siedenburg, Paul Schuladen, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Christoph Reuter' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>23rd International Congress on Acoustics</em>, 2019 </div> <div class="links"> <a href="http://pub.dega-akustik.de/ICA2019/data/articles/000813.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Abs</a> </div> <div class="abstract hidden"> <p>http://pub.dega-akustik.de/ICA2019/data/articles/000813.pdf</p> </div> </div> </div> </li> <li> <div class="row"> <div id="charis_smpc_2019b" class="col-12"> <div class="title">Revisiting timbral brightness perception</div> <div class="author"> Charalampos Saitis, Kai Siedenburg, and Christoph Reuter</div> <div class="periodical"> <em>Biennial Meeting of the Society for Music Perception and Cognition</em>, 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Brightness_SMPC_DGM_2019_v2.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>Brightness has been long shown to play a major role in timbre perception but relatively little is known about the specific acoustic and cognitive factors that affect brightness ratings of musical instrument sounds. Previous work indicated that sound source categories influence general timbre dissimilarity ratings. To examine whether source categories also exert an effect on brightness ratings of timbre, we collected brightness dissimilarity ratings of 14 orchestral instrument tones from 40 musically experienced listeners and the data were modeled using a partial least-squares regression model that takes audio descriptors of timbre as regressors. It was found that adding predictors derived from sound source categories did not improve the model fit, indicating that timbral brightness is informed mainly by continuously varying properties of the acoustic signal. A multidimensional scaling analysis suggested at least two salient cues: spectral energy distribution and attack time and/or asynchrony in the rise of harmonics. This finding seems to challenge the typical approach of seeking acoustical correlates of brightness in the spectral envelope of the steady-state portion of sounds. To further investigate these aspects in timbral brightness perception, a new group of 40 musically experienced listeners will perform MUSHRA-like brightness ratings of an expanded set of 24 orchestral instrument notes. The goal is to obtain a perceptual scaling of the attribute across a larger set of sounds to help delineate the acoustic ingredients of this important aspect of timbre perception. Preliminary results indicate that between sounds with very close spectral centroid values but different attack times, those with faster attacks tend to be perceived as brighter. Overall, these experiments help clarify the relation between two salient dimensions of timbre: onset and spectral energy distribution.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="charis_smpc_2019a" class="col-12"> <div class="title">There’s more to timbre than musical instruments: a meta-analysis of timbre semantics in singing voice quality perception</div> <div class="author"> Charalampos Saitis, and Johanna Devaney</div> <div class="periodical"> <em>Biennial Meeting of the Society for Music Perception and Cognition</em>, 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://osf.io/an57w" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a> </div> <div class="abstract hidden"> <p>Imagine listening to the famous soprano Maria Callas (1923–1977) singing the aria “Vissi d’arte” from Puccini’s Tosca. How would you describe the quality of her voice? When describing the timbre of musical sounds, listeners use descriptions such as bright, heavy, round, and rough, among others. In 1890, Stumpf theorized that this diverse vocabulary can be summarized, on the basis of semantic proximities, by three pairs of opposites: dark–bright, soft–rough, and full–empty. Empirical findings across many semantic differential studies from the late 1950s until today have generally confirmed that these are the salient dimensions of timbre semantics. However, most prior work has considered only orchestral instruments, with relatively little attention given to sung tones. At the same time, research on the perception of singing voice quality has primarily focused on verbal attributes associated with phonation type, voice classification, vocal register, vowel intelligibility, and vibrato. Descriptions like pressed, soprano, falsetto, hoarse, or wobble, albeit in themselves a type of timbre semantics, are essentially sound source identifiers acting as semantic descriptors. It remains an open question as to whether the timbral attributes of sung tones, that is verbal attributes that bear no source associations, can be described adequately on the basis of the bright-rough-full semantic space. We present a meta-analysis of previous research on verbal attributes of singing voice timbre that covers not only pedagogical texts but also work from music cognition, psychoacoustics, music information retrieval, musicology, and ethnomusicology. The meta-analysis lays the groundwork for a semantic differential study of sung sounds, providing a more appropriate lexicon on which to draw than simply using verbal scales from related work on instrumental timbre. The meta-analysis will be complemented by a psycholinguistic analysis of free verbalizations provided by singing teachers in a listening test and an acoustic analysis of the tested stimuli.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="charis_smpc_2019c" class="col-12"> <div class="title">Spectrotemporal modulation timbre cues in musical dynamics</div> <div class="author"> Charalampos Saitis, Luca Marinelli, Athanasios Lykartsis, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Stefan Weinzierl' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Biennial Meeting of the Society for Music Perception and Cognition</em>, 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Timbre is often described as a complex set of sound features that are not accounted for by pitch, loudness, duration, spatial location, and the acoustic environment. Musical dynamics refers to the perceived or intended loudness of a played note, instructed in music notation as piano or forte (soft or loud) with different dynamic gradations between and beyond. Recent research has shown that even if no loudness cues are available, listeners can still quite reliably identify the intended dynamic strength of a performed sound by relying on timbral features. More recently, acoustical analyses across an extensive set of anechoic recordings of orchestral instrument notes played at pianissimo (pp) and fortissimo (ff) showed that attack slope, spectral skewness, and spectral flatness together explained 72% of the variance in dynamic strength across all instruments, and 89% with an instrument-specific model. Here, we further investigate the role of timbre in musical dynamics, focusing specifically on the contribution of spectral and temporal modulations. Loudness-normalized modulation power spectra (MPS) were used as input representation for a convolutional neural network (CNN). Through visualization of the pp and ff saliency maps of the CNN it was possible to identify discriminant regions of the MPS and define a novel task-specific scalar audio descriptor. A linear discriminant analysis with 10-fold cross-validation using this new MPS-based descriptor on the entire dataset performed better than using the two spectral descriptors (27% error rate reduction). Overall, audio descriptors based on different regions of the MPS could serve as sound representation for machine listening applications, as well as to better delineate the acoustic ingredients of different aspects of auditory perception.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="charis_cmmr_2019" class="col-12"> <div class="title">Beyond the semantic differential: Timbre semantics as crossmodal correspondences</div> <div class="author"> Charalampos Saitis</div> <div class="periodical"> <em>14th International Symposium on Computer Music Multidisciplinary Research</em>, 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://hal.archives-ouvertes.fr/hal-02382500/document#page=344" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>This position paper argues that a systematic study of crossmodal correspondences between timbral dimensions of sound and perceptual dimensions of other sensory modalities (e.g., brightness, fullness, roughness, sweetness) can offer a new way of addressing old questions about the perceptual and cognitive mechanisms of timbre semantics, while the latter can provide a test case for better understanding crossmodal correspondences and human semantic processing in general. Furthermore, a systematic investigation of auditory-nonauditory crossmodal correspondences necessitates auditory stimuli that can be intuitively controlled along intrinsic continuous dimensions of timbre, and the collection of behavioural data from appropriate tasks that extend beyond the semantic differential paradigm.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="charis_isma_2019" class="col-12"> <div class="title">Sounds like melted chocolate: how musicians conceptualize violin sound richness</div> <div class="author"> Charalampos Saitis, Claudia Fritz, and Gary Scavone</div> <div class="periodical"> <em>International Symposium on Musical Acoustics</em>, 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/charis_ISMA_2019.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Results from a previous study on the perceptual evaluation of violins that involved playing-based semantic ratings showed that preference for a violin was strongly associated with its perceived sound richness. However, both preference and richness ratings varied widely between individual violinists, likely because musicians conceptualize the same attribute in different ways. To better understand how richness is conceptualized by violinists and how it contributes to the perceived quality of a violin, we analyzed free verbal descriptions collected during a carefully controlled playing task (involving 16 violinists) and in an online survey where no sound examples or other contextual information was present (involving 34 violinists). The analysis was based on a psycholinguistic method, whereby semantic categories are inferred from the verbal data itself through syntactic context and linguistic markers. The main sensory property related to violin sound richness was expressed through words such as full, complex, and dense versus thin and small, referring to the perceived number of partials present in the sound. Another sensory property was expressed through words such as warm, velvety, and smooth versus strident, harsh, and tinny, alluding to spectral energy distribution cues. Haptic cues were also implicated in the conceptualization of violin sound richness.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="Saitis_chap5" class="col-12"> <div class="title">The Semantics of Timbre</div> <div class="author"> Charalampos Saitis, and Stefan Weinzierl</div> <div class="periodical"> <em>Timbre: Acoustics, Perception, and Cognition</em>, 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Saitis_chap5.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Because humans lack a sensory vocabulary for auditory experiences, timbral qualities of sounds are often conceptualized and communicated through readily available sensory attributes from different modalities (e.g., bright, warm, sweet) but also through the use of onomatopoeic attributes (e.g., ringing, buzzing, shrill) or nonsensory attributes relating to abstract constructs (e.g., rich, complex, harsh). The analysis of the linguistic description of timbre, or timbre semantics, can be considered as one way to study its perceptual representation empirically. In the most commonly adopted approach, timbre is considered as a set of verbally defined perceptual attributes that represent the dimensions of a semantic timbre space. Previous studies have identified three salient semantic dimensions for timbre along with related acoustic properties. Comparisons with similarity-based multidimensional models confirm the strong link between perceiving timbre and talking about it. Still, the cognitive and neural mechanisms of timbre semantics remain largely unknown and underexplored, especially when one looks beyond the case of acoustic musical instruments.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="Timbre_chapter1" class="col-12"> <div class="title">The present, past, and future of timbre research</div> <div class="author"> Kai Siedenburg, Charalampos Saitis, and Stephen McAdams</div> <div class="periodical"> <em>Timbre: Acoustics, Perception, and Cognition</em>, 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Timbre_chapter1.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Timbre is a foundational aspect of hearing. The remarkable ability of humans to recognize sound sources and events (e.g., glass breaking, a friend’s voice, a tone from a piano) stems primarily from a capacity to perceive and process differences in the timbre of sounds. Roughly defined, timbre is thought of as any property other than pitch, duration, and loudness that allows two sounds to be distinguished. Current research unfolds along three main fronts: (1) principal perceptual and cognitive processes; (2) the role of timbre in human voice perception, perception through cochlear implants, music perception, sound quality, and sound design; and (3) computational acoustic modeling. Along these three scientific fronts, significant breakthroughs have been achieved during the decade prior to the production of this volume. Bringing together leading experts from around the world, this volume provides a joint forum for novel insights and the first comprehensive modern account of research topics and methods on the perception, cognition, and acoustic modeling of timbre. This chapter provides background information and a roadmap for the volume.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="Timbre_chapter11" class="col-12"> <div class="title">Audio Content Descriptors of Timbre</div> <div class="author"> Marcelo Caetano, Charalampos Saitis, and Kai Siedenburg</div> <div class="periodical"> <em>Timbre: Acoustics, Perception, and Cognition</em>, 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Caetano_chap11.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This chapter introduces acoustic modeling of timbre with the audio descriptors commonly used in music, speech, and environmental sound studies. These descriptors derive from different representations of sound, ranging from the waveform to sophisticated time-frequency transforms. Each representation is more appropriate for a specific aspect of sound description that is dependent on the information captured. Auditory models of both temporal and spectral information can be related to aspects of timbre perception, whereas the excitation-filter model of sound production provides links to the acoustics of sound production. A brief review of the most common representations of audio signals used to extract audio descriptors related to timbre is followed by a discussion of the audio descriptor extraction process using those representations. This chapter covers traditional temporal and spectral descriptors, including harmonic description, time-varying descriptors, and techniques for descriptor selection and descriptor decomposition. The discussion is focused on conceptual aspects of the acoustic modeling of timbre and the relationship between the descriptors and timbre perception, semantics, and cognition, including illustrative examples. The applications covered in this chapter range from timbre psychoacoustics and multimedia descriptions to computer-aided orchestration and sound morphing. Finally, the chapter concludes with speculation on the role of deep learning in the future of timbre description and on the challenges of audio content descriptors of timbre.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="Timbre_book" class="col-12"> <div class="title">Timbre: Acoustics, Perception, and Cognition</div> <div class="author"> Eds: Kai Siedenburg, Charalampos Saitis, Stephen McAdams, and <span class="more-authors" title="click to view 2 more editors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more editors' ? 'Arthur N. Popper, Richard R. Fay' : '2 more editors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more editors</span> </div> <div class="periodical"> Springer Handbook of Auditory Research 69, 2019 </div> <div class="links"> <a href="https://link.springer.com/book/10.1007/978-3-030-14832-4" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Link</a> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2025 Communication Acoustics Lab, Queen Mary University of London. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>